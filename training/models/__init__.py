"""
2048 CNN-DQN ëª¨ë¸ íŒ¨í‚¤ì§€

CNN ê¸°ë°˜ DQN ì—ì´ì „íŠ¸ì™€ ê´€ë ¨ëœ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
- networks: CNN ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ (layered ê´€ì°° ì „ìš©)
- dqn_agent: DQN ì—ì´ì „íŠ¸ (Double DQN, Dueling DQN ì§€ì›)
- replay_buffer: ê²½í—˜ ì¬ìƒ ë²„í¼ (ì¼ë°˜ ë° ìš°ì„ ìˆœìœ„)
"""

# í•µì‹¬ í´ë˜ìŠ¤ë“¤ import - ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œ import
from .networks import (
    CNN2048Network, 
    create_network, 
    count_parameters
)

from .replay_buffer import (
    ReplayBuffer, 
    PrioritizedReplayBuffer,
    Experience
)

from .dqn_agent import DQNAgent

__all__ = [
    # Networks
    'CNN2048Network',
    'create_network',
    'count_parameters',
    
    # Replay Buffers
    'ReplayBuffer',
    'PrioritizedReplayBuffer',
    'Experience',
    
    # Agent
    'DQNAgent'
]

DEFAULT_MODEL_CONFIG = {
    'observation_type': 'layered',  # CNN ì „ìš©
    'lr': 1e-4,
    'gamma': 0.99,
    'epsilon_start': 1.0,
    'epsilon_end': 0.01,
    'epsilon_decay': 50000,
    'buffer_size': 100000,
    'batch_size': 32,
    'target_update': 1000,
    'double_dqn': True,
    'dueling': True,
    'prioritized_replay': True
}

def create_agent(**kwargs):
    """CNN-DQN ì—ì´ì „íŠ¸ ìƒì„± íŒ©í† ë¦¬ í•¨ìˆ˜"""
    config = DEFAULT_MODEL_CONFIG.copy()
    config.update(kwargs)
    # observation_type ê°•ì œ ì„¤ì •
    config['observation_type'] = 'layered'
    return DQNAgent(**config)

SUPPORTED_NETWORKS = {
    'layered': 'CNN2048Network'
}

SUPPORTED_OBSERVATION_TYPES = ['layered']

def get_model_info():
    """ëª¨ë¸ íŒ¨í‚¤ì§€ ì •ë³´ ì¶œë ¥"""
    print("ğŸ“Š CNN-DQN ëª¨ë¸ íŒ¨í‚¤ì§€ ì •ë³´:")
    print(f"  ì§€ì› ë„¤íŠ¸ì›Œí¬: {list(SUPPORTED_NETWORKS.keys())}")
    print(f"  ì§€ì› ê´€ì°° íƒ€ì…: {SUPPORTED_OBSERVATION_TYPES}")
    print(f"  ê¸°ë³¸ ì„¤ì •: Double DQN, Dueling DQN, Prioritized Replay")

print(f"âœ… Models íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ - ì§€ì› ë„¤íŠ¸ì›Œí¬: {list(SUPPORTED_NETWORKS.keys())}")