import numpy as np
import torch
import random
from collections import deque, namedtuple
from typing import List, Tuple, Optional

# Experience tuple
Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class ReplayBuffer:
    """DQNìš© ê²½í—˜ ì¬ìƒ ë²„í¼"""
    
    def __init__(self, capacity: int, seed: Optional[int] = None):
        """
        Args:
            capacity: ë²„í¼ ìµœëŒ€ í¬ê¸°
            seed: ëœë¤ ì‹œë“œ
        """
        self.capacity = capacity  # ğŸ”¥ ì¶”ê°€: capacity ì†ì„±
        self.buffer = []
        self.position = 0
        
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
    
    def push(self, state: np.ndarray, action: int, reward: float, 
             next_state: np.ndarray, done: bool) -> None:
        """ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€"""
        experience = Experience(state, action, reward, next_state, done)
        self.buffer.append(experience)
    
    def sample(self, batch_size: int, device: torch.device = torch.device('cpu')) -> Tuple[torch.Tensor, ...]:
        """ë°°ì¹˜ ìƒ˜í”Œë§ - ì„±ëŠ¥ ìµœì í™” ë²„ì „"""
        if len(self.buffer) < batch_size:
            raise ValueError(f"ë²„í¼ í¬ê¸°({len(self.buffer)})ê°€ ë°°ì¹˜ í¬ê¸°({batch_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
        
        # ëœë¤ ìƒ˜í”Œë§
        experiences = random.sample(self.buffer, batch_size)
        
        # íš¨ìœ¨ì ì¸ í…ì„œ ë³€í™˜ - ê²½ê³  í•´ê²°
        states_list = [e.state for e in experiences]
        actions_list = [e.action for e in experiences]
        rewards_list = [e.reward for e in experiences]
        next_states_list = [e.next_state for e in experiences]
        dones_list = [e.done for e in experiences]
        
        # NumPy ë°°ì—´ë¡œ ë¨¼ì € ë³€í™˜ í›„ í…ì„œë¡œ ë³€í™˜ (ì„±ëŠ¥ ê°œì„ )
        states = torch.from_numpy(np.array(states_list, dtype=np.float32)).to(device)
        actions = torch.from_numpy(np.array(actions_list, dtype=np.int64)).to(device)
        rewards = torch.from_numpy(np.array(rewards_list, dtype=np.float32)).to(device)
        next_states = torch.from_numpy(np.array(next_states_list, dtype=np.float32)).to(device)
        # bool ê²½ê³  í•´ê²°: ëª…ì‹œì ìœ¼ë¡œ boolë¡œ ë³€í™˜
        dones = torch.from_numpy(np.array(dones_list, dtype=bool)).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self) -> int:
        """ë²„í¼ í¬ê¸° ë°˜í™˜"""
        return len(self.buffer)
    
    def is_ready(self, batch_size: int) -> bool:
        """ë°°ì¹˜ í¬ê¸°ë§Œí¼ ìƒ˜í”Œì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return len(self) >= batch_size

class PrioritizedReplayBuffer:
    """ìš°ì„ ìˆœìœ„ ê²½í—˜ ì¬ìƒ ë²„í¼"""
    
    def __init__(self, capacity: int, alpha: float = 0.6, beta: float = 0.4, 
                 beta_frames: int = 100000, seed: Optional[int] = None):
        """
        Args:
            capacity: ë²„í¼ ìµœëŒ€ í¬ê¸°
            alpha: ìš°ì„ ìˆœìœ„ ì§€ìˆ˜ (0=uniform, 1=full priority)
            beta: importance sampling ì‹œì‘ê°’
            beta_frames: betaê°€ 1ì— ë„ë‹¬í•˜ëŠ” í”„ë ˆì„ ìˆ˜
            seed: ëœë¤ ì‹œë“œ
        """
        self.capacity = capacity
        self.alpha = alpha
        self.beta_start = beta
        self.beta = beta
        self.beta_frames = beta_frames
        self.frame = 1
        
        # ğŸ”¥ ëˆ„ë½ëœ ì†ì„±ë“¤ ì¶”ê°€
        self.tree_capacity = 1
        while self.tree_capacity < capacity:
            self.tree_capacity *= 2
        
        self.tree = np.zeros(2 * self.tree_capacity - 1)
        self.data = np.empty(self.tree_capacity, dtype=object)
        self.data_pointer = 0
        self.size = 0  # ğŸ”¥ ì´ ì†ì„±ì´ ëˆ„ë½ë˜ì—ˆìŒ!
        
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
    
    def _update_tree(self, tree_index: int, priority: float):
        """íŠ¸ë¦¬ ì—…ë°ì´íŠ¸"""
        change = priority - self.tree[tree_index]
        self.tree[tree_index] = priority
        
        # ë¶€ëª¨ ë…¸ë“œë“¤ ì—…ë°ì´íŠ¸
        while tree_index != 0:
            tree_index = (tree_index - 1) // 2
            self.tree[tree_index] += change
    
    def _get_leaf(self, value: float) -> Tuple[int, float, int]:
        """ê°’ì— í•´ë‹¹í•˜ëŠ” ë¦¬í”„ ë…¸ë“œ ì°¾ê¸°"""
        parent_index = 0
        
        while True:
            left_child_index = 2 * parent_index + 1
            right_child_index = left_child_index + 1
            
            if left_child_index >= len(self.tree):
                leaf_index = parent_index
                break
            else:
                if value <= self.tree[left_child_index]:
                    parent_index = left_child_index
                else:
                    value -= self.tree[left_child_index]
                    parent_index = right_child_index
        
        data_index = leaf_index - self.tree_capacity + 1
        return leaf_index, self.tree[leaf_index], data_index
    
    def push(self, state: np.ndarray, action: int, reward: float,
             next_state: np.ndarray, done: bool, error: Optional[float] = None) -> None:
        """ê²½í—˜ì„ ë²„í¼ì— ì¶”ê°€"""
        experience = Experience(state, action, reward, next_state, done)
        
        # ìš°ì„ ìˆœìœ„ ê³„ì‚° (TD error ê¸°ë°˜)
        if error is None:
            priority = np.max(self.tree[-self.tree_capacity:]) if self.size > 0 else 1.0
        else:
            priority = (abs(error) + 1e-6) ** self.alpha
        
        # ë°ì´í„° ì €ì¥
        self.data[self.data_pointer] = experience
        tree_index = self.data_pointer + self.tree_capacity - 1
        self._update_tree(tree_index, priority)
        
        self.data_pointer = (self.data_pointer + 1) % self.tree_capacity
        if self.size < self.tree_capacity:
            self.size += 1
    
    def sample(self, batch_size: int, device: torch.device = torch.device('cpu')):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë°°ì¹˜ ìƒ˜í”Œë§"""
        if self.size < batch_size:
            raise ValueError(f"ë²„í¼ í¬ê¸°({self.size})ê°€ ë°°ì¹˜ í¬ê¸°({batch_size})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
        
        # Beta ìŠ¤ì¼€ì¤„ë§
        beta = min(1.0, self.beta_start + (1.0 - self.beta_start) * self.frame / self.beta_frames)
        self.frame += 1
        
        # ìƒ˜í”Œë§
        indices = []
        priorities = []
        segment = self.tree[0] / batch_size
        
        for i in range(batch_size):
            a = segment * i
            b = segment * (i + 1)
            value = random.uniform(a, b)
            index, priority, data_index = self._get_leaf(value)
            indices.append(data_index)
            priorities.append(priority)
        
        # Importance sampling weights
        sampling_probabilities = np.array(priorities, dtype=np.float64) / self.tree[0]
        is_weights = np.power(self.size * sampling_probabilities, -beta)
        is_weights /= is_weights.max()
        
        # ê²½í—˜ ì¶”ì¶œ
        experiences = [self.data[i] for i in indices]
        
        # íš¨ìœ¨ì ì¸ í…ì„œ ë³€í™˜
        states_list = [e.state for e in experiences]
        actions_list = [e.action for e in experiences]
        rewards_list = [e.reward for e in experiences]
        next_states_list = [e.next_state for e in experiences]
        dones_list = [e.done for e in experiences]
        
        states = torch.from_numpy(np.array(states_list, dtype=np.float32)).to(device)
        actions = torch.from_numpy(np.array(actions_list, dtype=np.int64)).to(device)
        rewards = torch.from_numpy(np.array(rewards_list, dtype=np.float32)).to(device)
        next_states = torch.from_numpy(np.array(next_states_list, dtype=np.float32)).to(device)
        dones = torch.from_numpy(np.array(dones_list, dtype=bool)).to(device)
        is_weights = torch.from_numpy(np.array(is_weights, dtype=np.float32)).to(device)
        
        return states, actions, rewards, next_states, dones, is_weights, indices
    
    def update_priorities(self, indices: List[int], errors: List[float]):
        """TD errorë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ì—…ë°ì´íŠ¸"""
        for idx, error in zip(indices, errors):
            priority = (abs(error) + 1e-6) ** self.alpha
            tree_index = idx + self.tree_capacity - 1
            self._update_tree(tree_index, priority)
    
    def __len__(self) -> int:
        """ë²„í¼ í¬ê¸° ë°˜í™˜"""
        return self.size
    
    def is_ready(self, batch_size: int) -> bool:
        """ë°°ì¹˜ í¬ê¸°ë§Œí¼ ìƒ˜í”Œì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.size >= batch_size

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_replay_buffer():
    """Replay Buffer í…ŒìŠ¤íŠ¸"""
    print("ğŸ’¾ Replay Buffer í…ŒìŠ¤íŠ¸")
    
    # ê¸°ë³¸ Replay Buffer í…ŒìŠ¤íŠ¸
    print("\n1. ê¸°ë³¸ Replay Buffer:")
    buffer = ReplayBuffer(capacity=1000)
    
    # ë”ë¯¸ ë°ì´í„° ì¶”ê°€
    for i in range(100):
        state = np.random.randn(16).astype(np.float32)  # ëª…ì‹œì  íƒ€ì… ì§€ì •
        action = int(np.random.randint(4))  # ëª…ì‹œì  int ë³€í™˜
        reward = float(np.random.randn())  # ëª…ì‹œì  float ë³€í™˜
        next_state = np.random.randn(16).astype(np.float32)
        done = bool(np.random.choice([True, False]))  # ëª…ì‹œì  bool ë³€í™˜
        buffer.push(state, action, reward, next_state, done)
    
    print(f"   - ë²„í¼ í¬ê¸°: {len(buffer)}")
    print(f"   - ìƒ˜í”Œë§ ì¤€ë¹„: {buffer.is_ready(32)}")
    
    # ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸
    states, actions, rewards, next_states, dones = buffer.sample(32)
    print(f"   - ìƒ˜í”Œ shapes: {states.shape}, {actions.shape}, {rewards.shape}")
    print(f"   - ë°ì´í„° íƒ€ì…: {states.dtype}, {actions.dtype}, {dones.dtype}")
    
    # ìš°ì„ ìˆœìœ„ Replay Buffer í…ŒìŠ¤íŠ¸
    print("\n2. ìš°ì„ ìˆœìœ„ Replay Buffer:")
    priority_buffer = PrioritizedReplayBuffer(capacity=1000)
    
    # ë”ë¯¸ ë°ì´í„° ì¶”ê°€
    for i in range(100):
        state = np.random.randn(16).astype(np.float32)
        action = int(np.random.randint(4))
        reward = float(np.random.randn())
        next_state = np.random.randn(16).astype(np.float32)
        done = bool(np.random.choice([True, False]))
        error = abs(float(np.random.randn()))  # TD error
        priority_buffer.push(state, action, reward, next_state, done, error)
    
    print(f"   - ë²„í¼ í¬ê¸°: {len(priority_buffer)}")
    print(f"   - ìƒ˜í”Œë§ ì¤€ë¹„: {priority_buffer.is_ready(32)}")
    
    # ìš°ì„ ìˆœìœ„ ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸
    result = priority_buffer.sample(32)
    states, actions, rewards, next_states, dones, is_weights, indices = result
    print(f"   - ìƒ˜í”Œ shapes: {states.shape}, {actions.shape}, {is_weights.shape}")
    print(f"   - IS weights ë²”ìœ„: [{is_weights.min():.3f}, {is_weights.max():.3f}]")
    print(f"   - ë°ì´í„° íƒ€ì…: {states.dtype}, {actions.dtype}, {dones.dtype}")
    
    print("\n ëª¨ë“  Replay Bufferê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")

if __name__ == "__main__":
    test_replay_buffer()