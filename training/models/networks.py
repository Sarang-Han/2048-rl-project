import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple

class CNN2048Network(nn.Module):
    """2048ìš© CNN ë„¤íŠ¸ì›Œí¬ - Layered ê´€ì°° íƒ€ì…ìš©"""
    
    def __init__(self, input_channels: int = 16, hidden_dim: int = 512):
        super(CNN2048Network, self).__init__()
        
        # Convolutional layers
        self.conv1 = nn.Conv2d(input_channels, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        
        # Batch normalization
        self.bn1 = nn.BatchNorm2d(128)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(128)
        
        # Calculate the size after convolutions (4x4 remains 4x4 with padding=1)
        conv_output_size = 4 * 4 * 128
        
        # Fully connected layers
        self.fc1 = nn.Linear(conv_output_size, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 4)  # 4 actions
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
        # Initialize weights
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ì‹ ê²½ë§ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        # Input shape: (batch_size, 4, 4, 16) -> (batch_size, 16, 4, 4)
        if x.dim() == 4 and x.shape[-1] == 16:
            x = x.permute(0, 3, 1, 2)
        
        # Convolutional layers with ReLU and batch normalization
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        
        # Flatten for fully connected layers - ìˆ˜ì •ëœ ë¶€ë¶„
        x = torch.flatten(x, start_dim=1)  # view() ëŒ€ì‹  flatten() ì‚¬ìš©
        
        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x

class DuelingDQNNetwork(nn.Module):
    """Dueling DQN ì•„í‚¤í…ì²˜ - ë” ì•ˆì •ì ì¸ í•™ìŠµ"""
    
    def __init__(self, base_network: nn.Module):
        super(DuelingDQNNetwork, self).__init__()
        
        self.base_network = base_network
        
        # Dueling ì•„í‚¤í…ì²˜ë¥¼ ìœ„í•´ base networkì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì œê±°
        # CNN ë„¤íŠ¸ì›Œí¬ì˜ ê²½ìš°
        hidden_dim = base_network.fc2.out_features
        base_network.fc3 = nn.Identity()
        
        # Value stream
        self.value_head = nn.Linear(hidden_dim, 1)
        
        # Advantage stream
        self.advantage_head = nn.Linear(hidden_dim, 4)
        
        # Initialize new heads
        nn.init.kaiming_normal_(self.value_head.weight, mode='fan_out', nonlinearity='relu')
        nn.init.constant_(self.value_head.bias, 0)
        nn.init.kaiming_normal_(self.advantage_head.weight, mode='fan_out', nonlinearity='relu')
        nn.init.constant_(self.advantage_head.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """ìˆœì „íŒŒ"""
        # Base features
        features = self.base_network(x)
        
        # Value and advantage
        value = self.value_head(features)
        advantage = self.advantage_head(features)
        
        # Dueling ê³µì‹: Q(s,a) = V(s) + A(s,a) - mean(A(s,a))
        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
        
        return q_values

def create_network(use_dueling: bool = True, **kwargs) -> nn.Module:
    """ë„¤íŠ¸ì›Œí¬ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    
    base_net = CNN2048Network(**kwargs)
    
    if use_dueling:
        return DuelingDQNNetwork(base_net)
    else:
        return base_net

def count_parameters(model: nn.Module) -> int:
    """ëª¨ë¸ì˜ ì´ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_networks():
    """ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  Neural Networks í…ŒìŠ¤íŠ¸")
    
    try:
        # CNN ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ (Layered ê´€ì°°)
        print("\n1. CNN Network (Layered observation):")
        cnn_net_dueling = create_network(use_dueling=True)
        cnn_net_simple = create_network(use_dueling=False)

        print(f"   - Dueling íŒŒë¼ë¯¸í„° ìˆ˜: {count_parameters(cnn_net_dueling):,}")
        print(f"   - Non-Dueling íŒŒë¼ë¯¸í„° ìˆ˜: {count_parameters(cnn_net_simple):,}")
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        layered_input = torch.randn(32, 4, 4, 16)  # batch_size=32
        print(f"   - ì…ë ¥ shape: {layered_input.shape}")
        
        cnn_output = cnn_net_dueling(layered_input)
        print(f"   - ì¶œë ¥ shape: {cnn_output.shape}")
        print(f"   - ì¶œë ¥ ë²”ìœ„: [{cnn_output.min():.3f}, {cnn_output.max():.3f}]")
        
        print("\nâœ… ëª¨ë“  ë„¤íŠ¸ì›Œí¬ê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_networks()
