{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70268a27",
      "metadata": {
        "id": "70268a27"
      },
      "source": [
        "# 2048 DQN ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "## í•™ìŠµ ê³„íš\n",
        "- **ëª©ì **: 2048 ê²Œì„ì—ì„œ ê³ ë“ì ì„ ë‹¬ì„±í•˜ëŠ” CNN ê¸°ë°˜ DQN ì—ì´ì „íŠ¸ í•™ìŠµ\n",
        "- **ì•„í‚¤í…ì²˜**: CNN (Layered)\n",
        "- **í™˜ê²½**: Google Colab GPU ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0f131c",
      "metadata": {
        "id": "cd0f131c"
      },
      "source": [
        "## í™˜ê²½ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea4d482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ea4d482",
        "outputId": "53deeb4c-75d6-41a9-ce45-fbeb1e371c40"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install torch torchvision gym matplotlib seaborn tensorboard\n",
        "!pip install onnx onnxruntime\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2aa566b",
      "metadata": {
        "id": "a2aa566b"
      },
      "source": [
        "## ì½”ë“œ ì—…ë¡œë“œ ë° ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e40528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e40528",
        "outputId": "f61d4581-0161-4846-a3bc-4f5bd3c284c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\n",
            "  í”„ë¡œì íŠ¸ ë£¨íŠ¸: /content/drive/MyDrive/2048-rl-project\n",
            "  í•™ìŠµ ê²½ë¡œ: /content/drive/MyDrive/2048-rl-project/training\n",
            "  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /content/drive/MyDrive/2048-rl-project/training\n"
          ]
        }
      ],
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/2048-rl-project'\n",
        "TRAINING_PATH = os.path.join(PROJECT_ROOT, 'training')\n",
        "\n",
        "# Python ê²½ë¡œì— ì¶”ê°€\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "if TRAINING_PATH not in sys.path:\n",
        "    sys.path.insert(0, TRAINING_PATH)\n",
        "\n",
        "# ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½\n",
        "os.chdir(TRAINING_PATH)\n",
        "\n",
        "print(f\"âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
        "print(f\"  í•™ìŠµ ê²½ë¡œ: {TRAINING_PATH}\")\n",
        "print(f\"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1d7c58",
      "metadata": {
        "id": "1a1d7c58"
      },
      "source": [
        "## ëª¨ë¸ ë° í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d975255d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d975255d",
        "outputId": "cde93c47-5b7e-425e-c783-e662a3e3521e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ëª¨ë“ˆ ë¡œë”© ì‹œì‘...\n",
            "âœ… Environment íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ - ì§€ì› ê´€ì°° íƒ€ì…: ['layered']\n",
            "âœ… Models íŒ¨í‚¤ì§€ ë¡œë“œ ì™„ë£Œ - ì§€ì› ë„¤íŠ¸ì›Œí¬: ['layered']\n",
            "âœ… ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# ëª¨ë“ˆ import ë° í…ŒìŠ¤íŠ¸\n",
        "print(\"ğŸ”„ ëª¨ë“ˆ ë¡œë”© ì‹œì‘...\")\n",
        "\n",
        "from environment.game_2048 import Game2048Env\n",
        "from models.dqn_agent import DQNAgent\n",
        "from models.networks import count_parameters\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "print(\"âœ… ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1de60ca",
      "metadata": {
        "id": "c1de60ca"
      },
      "source": [
        "## í•™ìŠµ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb624383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb624383",
        "outputId": "42e20213-5ba7-49ab-8872-9a03fc8e93de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:\n",
            "  episodes: 2000\n",
            "  max_steps_per_episode: 1000\n",
            "  device: cuda\n",
            "  buffer_size: 100000\n",
            "  batch_size: 64\n",
            "  lr: 0.0001\n",
            "  gamma: 0.99\n",
            "  epsilon_start: 1.0\n",
            "  epsilon_end: 0.01\n",
            "  epsilon_decay: 50000\n",
            "  target_update: 1000\n",
            "  eval_interval: 100\n",
            "  eval_episodes: 10\n",
            "  save_interval: 500\n",
            "  plot_interval: 50\n",
            "  log_interval: 10\n"
          ]
        }
      ],
      "source": [
        "TRAINING_CONFIG = {\n",
        "    # ê¸°ë³¸ ì„¤ì •\n",
        "    'episodes': 2000,\n",
        "    'max_steps_per_episode': 1000,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # DQN ì„¤ì •\n",
        "    'buffer_size': 100000,\n",
        "    'batch_size': 64,\n",
        "    'lr': 1e-4,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 0.9,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 30000,\n",
        "    'target_update': 1000,\n",
        "\n",
        "    # í‰ê°€ ë° ë¡œê¹… ì„¤ì •\n",
        "    'eval_interval': 100,\n",
        "    'eval_episodes': 10,\n",
        "    'save_interval': 500,\n",
        "    'plot_interval': 50,\n",
        "    'log_interval': 10\n",
        "}\n",
        "\n",
        "print(\"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:\")\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c94975f",
      "metadata": {
        "id": "9c94975f"
      },
      "source": [
        "## ëª¨ë¸ ì´ˆê¸°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fa33dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17fa33dd",
        "outputId": "3d636bab-2dd4-4cec-bf75-01c551263cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§  CNN ì—ì´ì „íŠ¸ ìƒì„±...\n",
            "ğŸ¤– DQN Agent ì´ˆê¸°í™” - Device: cuda\n",
            "ğŸ¤– DQN Agent ì´ˆê¸°í™” ì™„ë£Œ\n",
            "   - Double DQN: True\n",
            "   - Dueling DQN: False\n",
            "   - Prioritized Replay: True\n",
            "ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: 1,628,292\n"
          ]
        }
      ],
      "source": [
        "# ê°œì„ ëœ ì—ì´ì „íŠ¸ ìƒì„± í•¨ìˆ˜\n",
        "def create_agent_and_env():\n",
        "    \"\"\"ê°œì„ ëœ ì—ì´ì „íŠ¸ì™€ í™˜ê²½ ìƒì„±\"\"\"\n",
        "    env = Game2048Env()\n",
        "\n",
        "    agent = DQNAgent(\n",
        "        lr=TRAINING_CONFIG['lr'],\n",
        "        gamma=TRAINING_CONFIG['gamma'],\n",
        "        epsilon_start=TRAINING_CONFIG['epsilon_start'],\n",
        "        epsilon_end=TRAINING_CONFIG['epsilon_end'],\n",
        "        epsilon_decay=TRAINING_CONFIG['epsilon_decay'],\n",
        "        buffer_size=TRAINING_CONFIG['buffer_size'],\n",
        "        batch_size=TRAINING_CONFIG['batch_size'],\n",
        "        target_update=TRAINING_CONFIG['target_update'],\n",
        "        double_dqn=True,           # Double DQN í™œì„±í™”\n",
        "        dueling=False,             # dueling head í™œì„±í™” ì—¬ë¶€\n",
        "        prioritized_replay=True,   # ì„±ëŠ¥ í–¥ìƒ\n",
        "        device=TRAINING_CONFIG['device']\n",
        "    )\n",
        "\n",
        "    return agent, env\n",
        "\n",
        "print(\"ğŸ§  CNN ì—ì´ì „íŠ¸ ìƒì„±...\")\n",
        "agent, env = create_agent_and_env()\n",
        "print(f\"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {count_parameters(agent.q_network):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e08891f",
      "metadata": {
        "id": "9e08891f"
      },
      "source": [
        "## í•™ìŠµ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6faa7ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6faa7ae",
        "outputId": "5221f46c-21f0-41d2-f2b1-ddf8bacb508e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "class TrainingMonitor:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.episode_rewards = []\n",
        "        self.episode_scores = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_losses = []\n",
        "        self.highest_tiles = []\n",
        "        self.eval_scores = []\n",
        "        self.eval_episodes = []\n",
        "\n",
        "    def add_episode(self, reward, score, steps, loss, highest_tile):\n",
        "        self.episode_rewards.append(reward)\n",
        "        self.episode_scores.append(score)\n",
        "        self.episode_steps.append(steps)\n",
        "        self.episode_losses.append(loss)\n",
        "        self.highest_tiles.append(highest_tile)\n",
        "\n",
        "    def add_eval(self, episode, avg_score):\n",
        "        self.eval_episodes.append(episode)\n",
        "        self.eval_scores.append(avg_score)\n",
        "\n",
        "    def plot_progress(self, title=\"Training Progress\"):\n",
        "        if len(self.episode_rewards) < 10:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "        # ë³´ìƒ\n",
        "        axes[0,0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
        "        axes[0,0].plot(pd.Series(self.episode_rewards).rolling(50).mean(), color='red')\n",
        "        axes[0,0].set_title('Episode Rewards')\n",
        "        axes[0,0].set_ylabel('Reward')\n",
        "\n",
        "        # ì ìˆ˜\n",
        "        axes[0,1].plot(self.episode_scores, alpha=0.3, color='green')\n",
        "        axes[0,1].plot(pd.Series(self.episode_scores).rolling(50).mean(), color='red')\n",
        "        axes[0,1].set_title('Episode Scores')\n",
        "        axes[0,1].set_ylabel('Score')\n",
        "\n",
        "        # ìµœê³  íƒ€ì¼\n",
        "        axes[0,2].plot(self.highest_tiles, alpha=0.3, color='purple')\n",
        "        axes[0,2].plot(pd.Series(self.highest_tiles).rolling(50).mean(), color='red')\n",
        "        axes[0,2].set_title('Highest Tiles')\n",
        "        axes[0,2].set_ylabel('Tile Value')\n",
        "\n",
        "        # ìŠ¤í… ìˆ˜\n",
        "        axes[1,0].plot(self.episode_steps, alpha=0.3, color='orange')\n",
        "        axes[1,0].plot(pd.Series(self.episode_steps).rolling(50).mean(), color='red')\n",
        "        axes[1,0].set_title('Episode Steps')\n",
        "        axes[1,0].set_ylabel('Steps')\n",
        "        axes[1,0].set_xlabel('Episode')\n",
        "\n",
        "        # ì†ì‹¤\n",
        "        if self.episode_losses and any(loss is not None for loss in self.episode_losses):\n",
        "            valid_losses = [l for l in self.episode_losses if l is not None]\n",
        "            if valid_losses:\n",
        "                axes[1,1].plot(valid_losses, alpha=0.3, color='red')\n",
        "                axes[1,1].plot(pd.Series(valid_losses).rolling(20).mean(), color='darkred')\n",
        "        axes[1,1].set_title('Training Loss')\n",
        "        axes[1,1].set_ylabel('Loss')\n",
        "        axes[1,1].set_xlabel('Episode')\n",
        "\n",
        "        # í‰ê°€ ì ìˆ˜\n",
        "        if self.eval_scores:\n",
        "            axes[1,2].plot(self.eval_episodes, self.eval_scores, 'o-', color='darkgreen')\n",
        "        axes[1,2].set_title('Evaluation Scores')\n",
        "        axes[1,2].set_ylabel('Avg Score')\n",
        "        axes[1,2].set_xlabel('Episode')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.episode_scores:\n",
        "            return {}\n",
        "\n",
        "        recent_scores = self.episode_scores[-100:]\n",
        "        recent_tiles = self.highest_tiles[-100:]\n",
        "\n",
        "        return {\n",
        "            'episodes': len(self.episode_scores),\n",
        "            'avg_score': np.mean(recent_scores),\n",
        "            'max_score': max(self.episode_scores),\n",
        "            'avg_highest_tile': np.mean(recent_tiles),\n",
        "            'max_highest_tile': max(self.highest_tiles),\n",
        "            'avg_steps': np.mean(self.episode_steps[-100:])\n",
        "        }\n",
        "\n",
        "monitor = TrainingMonitor()\n",
        "print(\"ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d8e403",
      "metadata": {
        "id": "e3d8e403"
      },
      "source": [
        "## í•™ìŠµ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1a6fb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1a6fb7",
        "outputId": "c43b669a-0e97-46a5-b214-b885dee5183e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”§ í•™ìŠµ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_agent(agent, env, num_episodes=5):\n",
        "    \"\"\"ì•¡ì…˜ ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
        "    total_scores = []\n",
        "    total_steps = []\n",
        "    highest_tiles = []\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        steps = 0\n",
        "        while steps < 1000:\n",
        "            # ğŸ”¥ ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            if not valid_actions:  # ê²Œì„ ì¢…ë£Œ (ì•ˆì „ì¥ì¹˜)\n",
        "                break\n",
        "                \n",
        "            action = agent.select_action(state, training=False, valid_actions=valid_actions)\n",
        "            next_state, _, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                total_scores.append(info['score'])\n",
        "                total_steps.append(steps)\n",
        "                highest_tiles.append(info['highest'])\n",
        "                break\n",
        "    \n",
        "    return {\n",
        "        'avg_score': np.mean(total_scores) if total_scores else 0,\n",
        "        'avg_steps': np.mean(total_steps) if total_steps else 0,\n",
        "        'avg_highest': np.mean(highest_tiles) if highest_tiles else 0,\n",
        "        'max_score': max(total_scores) if total_scores else 0,\n",
        "        'max_highest': max(highest_tiles) if highest_tiles else 0\n",
        "    }\n",
        "\n",
        "def train_agent(agent, env, monitor, episodes):\n",
        "    \"\"\"ì•¡ì…˜ ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)\"\"\"\n",
        "    print(f\"ğŸš€ ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš© CNN ëª¨ë¸ í•™ìŠµ ì‹œì‘! (ëª©í‘œ: {episodes} ì—í”¼ì†Œë“œ)\")\n",
        "    start_time = time.time()\n",
        "    best_score = 0\n",
        "    best_highest = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        episode_losses = []\n",
        "        final_info = {'score': 0, 'highest': 0, 'valid_actions': []}  # ê¸°ë³¸ê°’ ì„¤ì •\n",
        "\n",
        "        while steps < TRAINING_CONFIG['max_steps_per_episode']:\n",
        "            # ğŸ”¥ ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            \n",
        "            # ìœ íš¨í•œ ì•¡ì…˜ì´ ì—†ìœ¼ë©´ ê²Œì„ ì¢…ë£Œ\n",
        "            if not valid_actions:\n",
        "                print(f\"âš ï¸ Episode {episode+1}: No valid actions available at step {steps}\")\n",
        "                break\n",
        "            \n",
        "            # ì•¡ì…˜ ì„ íƒ (ìœ íš¨í•œ ì•¡ì…˜ë§Œ ê³ ë ¤)\n",
        "            action = agent.select_action(state, training=True, valid_actions=valid_actions)\n",
        "            \n",
        "            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            final_info = info  # ë§ˆì§€ë§‰ info ì €ì¥\n",
        "            \n",
        "            # ê²½í—˜ ì €ì¥\n",
        "            if hasattr(agent, 'prioritized_replay') and agent.prioritized_replay and agent.memory.is_ready(agent.batch_size):\n",
        "                # TD error ë¯¸ë¦¬ ê³„ì‚°\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
        "                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(agent.device)\n",
        "                    current_q = agent.q_network(state_tensor)[0][action]\n",
        "                    next_q = agent.target_network(next_state_tensor).max(1)[0]\n",
        "                    target_q = reward + (agent.gamma * next_q * (not done))\n",
        "                    td_error = abs((target_q - current_q).item())\n",
        "                    agent.store_experience(state, action, reward, next_state, done, td_error)\n",
        "            else:\n",
        "                agent.store_experience(state, action, reward, next_state, done)\n",
        "\n",
        "            # í•™ìŠµ ìˆ˜í–‰\n",
        "            if agent.memory.is_ready(agent.batch_size):\n",
        "                loss = agent.train_step()\n",
        "                if loss is not None:\n",
        "                    episode_losses.append(loss)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # ì—í”¼ì†Œë“œ í†µê³„ ì—…ë°ì´íŠ¸\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else None\n",
        "        monitor.add_episode(total_reward, final_info['score'], steps, avg_loss, final_info['highest'])\n",
        "        agent.episode_rewards.append(total_reward)\n",
        "\n",
        "        # ìµœê³  ê¸°ë¡ ê°±ì‹ \n",
        "        if final_info['score'] > best_score:\n",
        "            best_score = final_info['score']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_score.pth')\n",
        "            \n",
        "        if final_info['highest'] > best_highest:\n",
        "            best_highest = final_info['highest']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_tile.pth')\n",
        "\n",
        "        # ì£¼ê¸°ì  ë¡œê¹…\n",
        "        if (episode + 1) % TRAINING_CONFIG['log_interval'] == 0:\n",
        "            stats = agent.get_stats()\n",
        "            elapsed = time.time() - start_time\n",
        "            loss_str = f\"{avg_loss:.4f}\" if avg_loss is not None else \"N/A\"\n",
        "            \n",
        "            print(f\"Episode {episode+1:4d} | Score: {final_info['score']:5.0f} | Highest: {final_info['highest']:4.0f} | \"\n",
        "                  f\"Reward: {total_reward:7.2f} | Steps: {steps:3d} | Valid: {len(final_info['valid_actions'])} | \"\n",
        "                  f\"Îµ: {stats['epsilon']:.3f} | Loss: {loss_str} | Time: {elapsed/60:.1f}min\")\n",
        "            \n",
        "        if (episode + 1) % TRAINING_CONFIG['eval_interval'] == 0:\n",
        "            eval_results = evaluate_agent(agent, env, TRAINING_CONFIG['eval_episodes'])\n",
        "            monitor.add_eval(episode + 1, eval_results['avg_score'])\n",
        "            \n",
        "            print(f\"ğŸ¯ í‰ê°€ ê²°ê³¼ (Episode {episode+1}):\")\n",
        "            print(f\"   í‰ê·  ì ìˆ˜: {eval_results['avg_score']:.1f}\")\n",
        "            print(f\"   ìµœê³  ì ìˆ˜: {eval_results['max_score']:.0f}\")\n",
        "            print(f\"   í‰ê·  ìµœê³ íƒ€ì¼: {eval_results['avg_highest']:.0f}\")\n",
        "            print(f\"   ìµœê³  íƒ€ì¼: {eval_results['max_highest']:.0f}\")\n",
        "\n",
        "        # ì£¼ê¸°ì  ì‹œê°í™”\n",
        "        if (episode + 1) % TRAINING_CONFIG['plot_interval'] == 0:\n",
        "            clear_output(wait=True)\n",
        "            monitor.plot_progress(f\"CNN Training Progress - Episode {episode+1}\")\n",
        "            \n",
        "            stats = monitor.get_stats()\n",
        "            print(f\"ğŸ“Š í˜„ì¬ í†µê³„ (ìµœê·¼ 100 ì—í”¼ì†Œë“œ):\")\n",
        "            for key, value in stats.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.2f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "            \n",
        "            print(f\"  í˜„ì¬ epsilon: {agent.get_epsilon():.4f}\")\n",
        "            print(f\"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {len(agent.memory):,}/{agent.memory.capacity:,}\")\n",
        "\n",
        "        # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥\n",
        "        if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_checkpoint_{episode+1}.pth')\n",
        "\n",
        "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "    agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_final_action_masked.pth')\n",
        "    \n",
        "    print(f\"\\nâœ… ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš© í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ† ìµœê³  ì ìˆ˜: {best_score}\")\n",
        "    print(f\"ğŸ¯ ìµœê³  íƒ€ì¼: {best_highest}\")\n",
        "    \n",
        "    return monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35c846c",
      "metadata": {
        "id": "f35c846c"
      },
      "source": [
        "## CNN ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bf4881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "73bf4881",
        "outputId": "015c3dff-d775-45d5-cd1a-7f201d50c56b"
      },
      "outputs": [],
      "source": [
        "monitor = train_agent(\n",
        "    agent=agent,\n",
        "    env=env,\n",
        "    monitor=monitor,\n",
        "    episodes=TRAINING_CONFIG['episodes']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0191a35b",
      "metadata": {
        "id": "0191a35b"
      },
      "source": [
        "## ğŸ† ìµœì¢… ì„±ëŠ¥ í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff302daf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff302daf",
        "outputId": "78c7c80d-e41a-4e72-9fc8-77a8c747b339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ† ìµœì¢… ì„±ëŠ¥ í‰ê°€ (50 ê²Œì„)\n",
            "ğŸ“Š ìµœì¢… í‰ê·  ì ìˆ˜: 572.4\n",
            "ğŸ“ˆ ìµœì¢… í•™ìŠµ í†µê³„:\n",
            "  episodes: 2000\n",
            "  avg_score: 716.32\n",
            "  max_score: 2548\n",
            "  avg_highest_tile: 77.12\n",
            "  max_highest_tile: 256\n",
            "  avg_steps: 90.04\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ† ìµœì¢… ì„±ëŠ¥ í‰ê°€ (50 ê²Œì„)\")\n",
        "final_results = evaluate_agent(agent, env, 50)\n",
        "print(f\"ğŸ“Š ìµœì¢… í‰ê·  ì ìˆ˜: {final_results['avg_score']:.1f}\")\n",
        "print(f\"ğŸ† ìµœê³  ì ìˆ˜: {final_results['max_score']:.0f}\")\n",
        "print(f\"ğŸ¯ í‰ê·  ìµœê³  íƒ€ì¼: {final_results['avg_highest']:.0f}\")\n",
        "print(f\"ğŸŒŸ ìµœê³  íƒ€ì¼: {final_results['max_highest']:.0f}\")\n",
        "\n",
        "stats = monitor.get_stats()\n",
        "print(\"\\nğŸ“ˆ ìµœì¢… í•™ìŠµ í†µê³„:\")\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fff8962",
      "metadata": {
        "id": "0fff8962"
      },
      "source": [
        "## ONNX ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0b97a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0b97a7",
        "outputId": "12ccaca5-9101-418c-d0d5-4675fa248d0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/2048-rl-project/training/models/networks.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if x.dim() == 4 and x.shape[-1] == 16:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ ONNX ëª¨ë¸ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: /content/drive/MyDrive/2048_models/cnn_model.onnx\n",
            "âœ… ONNX ë³€í™˜ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...\")\n",
        "agent.export_to_onnx(\n",
        "    filepath='/content/drive/MyDrive/2048_models/cnn_model.onnx',\n",
        "    input_shape=(4, 4, 16)\n",
        ")\n",
        "print(\"âœ… ONNX ë³€í™˜ ì™„ë£Œ!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
