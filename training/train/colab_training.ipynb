{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70268a27",
   "metadata": {},
   "source": [
    "# 2048 DQN ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "## í•™ìŠµ ê³„íš\n",
    "- **ëª©ì **: 2048 ê²Œì„ì—ì„œ ê³ ë“ì ì„ ë‹¬ì„±í•˜ëŠ” DQN ì—ì´ì „íŠ¸ í•™ìŠµ\n",
    "- **ì•„í‚¤í…ì²˜**: CNN (Layered) vs DNN (Flat) ë¹„êµ\n",
    "- **í™˜ê²½**: Google Colab GPU ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f131c",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install torch torchvision gym matplotlib seaborn tensorboard\n",
    "!pip install onnx onnxruntime\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa566b",
   "metadata": {},
   "source": [
    "## ì½”ë“œ ì—…ë¡œë“œ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e40528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/2048-rl-project'\n",
    "TRAINING_PATH = os.path.join(PROJECT_ROOT, 'training')\n",
    "\n",
    "# Python ê²½ë¡œì— ì¶”ê°€\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if TRAINING_PATH not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_PATH)\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½\n",
    "os.chdir(TRAINING_PATH)\n",
    "\n",
    "# __init__.py íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "init_files = [\n",
    "    os.path.join(PROJECT_ROOT, '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'models', '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'environment', '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'train', '__init__.py')\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ __init__.py íŒŒì¼ í™•ì¸:\")\n",
    "for init_file in init_files:\n",
    "    exists = \"âœ…\" if os.path.exists(init_file) else \"âŒ\"\n",
    "    print(f\"  {exists} {init_file}\")\n",
    "\n",
    "print(f\"\\nâœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
    "print(f\"  í•™ìŠµ ê²½ë¡œ: {TRAINING_PATH}\")\n",
    "print(f\"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d7c58",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ë° í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë“ˆ import ë° í…ŒìŠ¤íŠ¸\n",
    "print(\"ğŸ”„ ëª¨ë“ˆ ë¡œë”© ì‹œì‘...\")\n",
    "\n",
    "try:\n",
    "    from environment.game_2048 import Game2048Env\n",
    "    print(\"âœ… Game2048Env ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Game2048Env ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "try:\n",
    "    from models.dqn_agent import DQNAgent\n",
    "    print(\"âœ… DQNAgent ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ DQNAgent ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "try:\n",
    "    from models.networks import count_parameters\n",
    "    print(\"âœ… Networks ë¡œë“œ ì„±ê³µ\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Networks ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# ê¸°íƒ€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "import pandas as pd\n",
    "\n",
    "# ì‹œê°í™” ì„¤ì •\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ë™ì‘ í…ŒìŠ¤íŠ¸\n",
    "print(\"\\nğŸ§ª ë™ì‘ í…ŒìŠ¤íŠ¸:\")\n",
    "try:\n",
    "    # í™˜ê²½ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "    test_env = Game2048Env(observation_type='flat')\n",
    "    print(f\"âœ… í™˜ê²½ ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ: {test_env.observation_space}\")\n",
    "    \n",
    "    # ì—ì´ì „íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸\n",
    "    test_agent = DQNAgent(observation_type='flat', buffer_size=1000, batch_size=32)\n",
    "    print(f\"âœ… ì—ì´ì „íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ: {count_parameters(test_agent.q_network):,} íŒŒë¼ë¯¸í„°\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ëª¨ë“ˆ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ“‹ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\")\n",
    "    print(\"  1. ëª¨ë“  íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆëŠ”ì§€\")\n",
    "    print(\"  2. __init__.py íŒŒì¼ë“¤ì´ ëª¨ë‘ ìƒì„±ë˜ì—ˆëŠ”ì§€\")\n",
    "    print(\"  3. import ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de60ca",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb624383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "TRAINING_CONFIG = {\n",
    "    # ê¸°ë³¸ ì„¤ì •\n",
    "    'episodes': 2000,\n",
    "    'max_steps_per_episode': 1000,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # DQN ì„¤ì •\n",
    "    'buffer_size': 100000,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-4,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 50000,\n",
    "    'target_update': 1000,\n",
    "    \n",
    "    # í‰ê°€ ì„¤ì •\n",
    "    'eval_interval': 100,\n",
    "    'eval_episodes': 10,\n",
    "    'save_interval': 500,\n",
    "    \n",
    "    # ì‹œê°í™” ì„¤ì •\n",
    "    'plot_interval': 50,\n",
    "    'log_interval': 10\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ í•™ìŠµ ì„¤ì • ì™„ë£Œ:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94975f",
   "metadata": {},
   "source": [
    "## ëª¨ë¸ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_and_env(observation_type='layered'):\n",
    "    \"\"\"ì—ì´ì „íŠ¸ì™€ í™˜ê²½ ìƒì„±\"\"\"\n",
    "    env = Game2048Env(observation_type=observation_type)\n",
    "    \n",
    "    agent = DQNAgent(\n",
    "        observation_type=observation_type,\n",
    "        lr=TRAINING_CONFIG['lr'],\n",
    "        gamma=TRAINING_CONFIG['gamma'],\n",
    "        epsilon_start=TRAINING_CONFIG['epsilon_start'],\n",
    "        epsilon_end=TRAINING_CONFIG['epsilon_end'],\n",
    "        epsilon_decay=TRAINING_CONFIG['epsilon_decay'],\n",
    "        buffer_size=TRAINING_CONFIG['buffer_size'],\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        target_update=TRAINING_CONFIG['target_update'],\n",
    "        device=TRAINING_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# CNNê³¼ DNN ì—ì´ì „íŠ¸ ìƒì„±\n",
    "print(\"ğŸ§  CNN ì—ì´ì „íŠ¸ (Layered) ìƒì„±...\")\n",
    "cnn_agent, cnn_env = create_agent_and_env('layered')\n",
    "\n",
    "print(\"\\nğŸ§  DNN ì—ì´ì „íŠ¸ (Flat) ìƒì„±...\")\n",
    "dnn_agent, dnn_env = create_agent_and_env('flat')\n",
    "\n",
    "print(f\"\\nğŸ“Š ëª¨ë¸ ë¹„êµ:\")\n",
    "print(f\"  CNN íŒŒë¼ë¯¸í„°: {count_parameters(cnn_agent.q_network):,}\")\n",
    "print(f\"  DNN íŒŒë¼ë¯¸í„°: {count_parameters(dnn_agent.q_network):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08891f",
   "metadata": {},
   "source": [
    "## í•™ìŠµ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faa7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_rewards = []\n",
    "        self.episode_scores = []\n",
    "        self.episode_steps = []\n",
    "        self.episode_losses = []\n",
    "        self.highest_tiles = []\n",
    "        self.eval_scores = []\n",
    "        self.eval_episodes = []\n",
    "    \n",
    "    def add_episode(self, reward, score, steps, loss, highest_tile):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_scores.append(score)\n",
    "        self.episode_steps.append(steps)\n",
    "        self.episode_losses.append(loss)\n",
    "        self.highest_tiles.append(highest_tile)\n",
    "    \n",
    "    def add_eval(self, episode, avg_score):\n",
    "        self.eval_episodes.append(episode)\n",
    "        self.eval_scores.append(avg_score)\n",
    "    \n",
    "    def plot_progress(self, title=\"Training Progress\"):\n",
    "        if len(self.episode_rewards) < 10:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        \n",
    "        # ë³´ìƒ\n",
    "        axes[0,0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
    "        axes[0,0].plot(pd.Series(self.episode_rewards).rolling(50).mean(), color='red')\n",
    "        axes[0,0].set_title('Episode Rewards')\n",
    "        axes[0,0].set_ylabel('Reward')\n",
    "        \n",
    "        # ì ìˆ˜\n",
    "        axes[0,1].plot(self.episode_scores, alpha=0.3, color='green')\n",
    "        axes[0,1].plot(pd.Series(self.episode_scores).rolling(50).mean(), color='red')\n",
    "        axes[0,1].set_title('Episode Scores')\n",
    "        axes[0,1].set_ylabel('Score')\n",
    "        \n",
    "        # ìµœê³  íƒ€ì¼\n",
    "        axes[0,2].plot(self.highest_tiles, alpha=0.3, color='purple')\n",
    "        axes[0,2].plot(pd.Series(self.highest_tiles).rolling(50).mean(), color='red')\n",
    "        axes[0,2].set_title('Highest Tiles')\n",
    "        axes[0,2].set_ylabel('Tile Value')\n",
    "        \n",
    "        # ìŠ¤í… ìˆ˜\n",
    "        axes[1,0].plot(self.episode_steps, alpha=0.3, color='orange')\n",
    "        axes[1,0].plot(pd.Series(self.episode_steps).rolling(50).mean(), color='red')\n",
    "        axes[1,0].set_title('Episode Steps')\n",
    "        axes[1,0].set_ylabel('Steps')\n",
    "        axes[1,0].set_xlabel('Episode')\n",
    "        \n",
    "        # ì†ì‹¤\n",
    "        if self.episode_losses and any(loss is not None for loss in self.episode_losses):\n",
    "            valid_losses = [l for l in self.episode_losses if l is not None]\n",
    "            if valid_losses:\n",
    "                axes[1,1].plot(valid_losses, alpha=0.3, color='red')\n",
    "                axes[1,1].plot(pd.Series(valid_losses).rolling(20).mean(), color='darkred')\n",
    "        axes[1,1].set_title('Training Loss')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        axes[1,1].set_xlabel('Episode')\n",
    "        \n",
    "        # í‰ê°€ ì ìˆ˜\n",
    "        if self.eval_scores:\n",
    "            axes[1,2].plot(self.eval_episodes, self.eval_scores, 'o-', color='darkgreen')\n",
    "        axes[1,2].set_title('Evaluation Scores')\n",
    "        axes[1,2].set_ylabel('Avg Score')\n",
    "        axes[1,2].set_xlabel('Episode')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        if not self.episode_scores:\n",
    "            return {}\n",
    "        \n",
    "        recent_scores = self.episode_scores[-100:] if len(self.episode_scores) >= 100 else self.episode_scores\n",
    "        recent_tiles = self.highest_tiles[-100:] if len(self.highest_tiles) >= 100 else self.highest_tiles\n",
    "        \n",
    "        return {\n",
    "            'episodes': len(self.episode_scores),\n",
    "            'avg_score': np.mean(recent_scores),\n",
    "            'max_score': max(self.episode_scores),\n",
    "            'avg_highest_tile': np.mean(recent_tiles),\n",
    "            'max_highest_tile': max(self.highest_tiles),\n",
    "            'avg_steps': np.mean(self.episode_steps[-100:]) if len(self.episode_steps) >= 100 else np.mean(self.episode_steps)\n",
    "        }\n",
    "\n",
    "# ëª¨ë‹ˆí„° ì´ˆê¸°í™”\n",
    "cnn_monitor = TrainingMonitor()\n",
    "dnn_monitor = TrainingMonitor()\n",
    "\n",
    "print(\"ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8e403",
   "metadata": {},
   "source": [
    "## í•™ìŠµ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, monitor, name, episodes):\n",
    "    \"\"\"ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜\"\"\"\n",
    "    print(f\"ğŸš€ {name} í•™ìŠµ ì‹œì‘! (ëª©í‘œ: {episodes} ì—í”¼ì†Œë“œ)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_score = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        while steps < TRAINING_CONFIG['max_steps_per_episode']:\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                loss = agent.train_step()\n",
    "                if loss is not None:\n",
    "                    episode_losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # ì—í”¼ì†Œë“œ í†µê³„ ê¸°ë¡\n",
    "        avg_loss = np.mean(episode_losses) if episode_losses else None\n",
    "        monitor.add_episode(total_reward, info['score'], steps, avg_loss, info['highest'])\n",
    "        agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥\n",
    "        if info['score'] > best_score:\n",
    "            best_score = info['score']\n",
    "            try:\n",
    "                agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_best.pth')\n",
    "            except Exception as save_error:\n",
    "                print(f\"âš ï¸ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {save_error}\")\n",
    "        \n",
    "        # ë¡œê·¸ ì¶œë ¥ - ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "        if (episode + 1) % TRAINING_CONFIG['log_interval'] == 0:\n",
    "            try:\n",
    "                stats = agent.get_stats()\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # ì†ì‹¤ ê°’ì„ ë¯¸ë¦¬ í¬ë§·íŒ… (ì•ˆì „í•œ ë°©ë²•)\n",
    "                if avg_loss is not None:\n",
    "                    loss_str = f\"{avg_loss:.3f}\"\n",
    "                else:\n",
    "                    loss_str = \"N/A\"\n",
    "                \n",
    "                print(f\"{name} Episode {episode+1:4d} | \"\n",
    "                      f\"Score: {info['score']:4.0f} | \"\n",
    "                      f\"Reward: {total_reward:6.1f} | \"\n",
    "                      f\"Steps: {steps:3d} | \"\n",
    "                      f\"Epsilon: {stats['epsilon']:.3f} | \"\n",
    "                      f\"Loss: {loss_str} | \"\n",
    "                      f\"Time: {elapsed/60:.1f}min\")\n",
    "                      \n",
    "            except Exception as log_error:\n",
    "                print(f\"âš ï¸ ë¡œê·¸ ì¶œë ¥ ì—ëŸ¬: {log_error}\")\n",
    "        \n",
    "        # í‰ê°€ ë° ì‹œê°í™”\n",
    "        if (episode + 1) % TRAINING_CONFIG['eval_interval'] == 0:\n",
    "            try:\n",
    "                eval_score = evaluate_agent(agent, env, TRAINING_CONFIG['eval_episodes'])\n",
    "                monitor.add_eval(episode + 1, eval_score)\n",
    "                print(f\"ğŸ¯ {name} Evaluation (Episode {episode+1}): {eval_score:.1f}\")\n",
    "            except Exception as eval_error:\n",
    "                print(f\"âš ï¸ í‰ê°€ ì—ëŸ¬: {eval_error}\")\n",
    "        \n",
    "        if (episode + 1) % TRAINING_CONFIG['plot_interval'] == 0:\n",
    "            try:\n",
    "                clear_output(wait=True)\n",
    "                monitor.plot_progress(f\"{name} Training Progress\")\n",
    "                \n",
    "                stats = monitor.get_stats()\n",
    "                print(f\"\\nğŸ“Š {name} í˜„ì¬ í†µê³„:\")\n",
    "                \n",
    "                # ì•ˆì „í•œ í†µê³„ ì¶œë ¥ - ìˆ˜ì •ëœ ë¶€ë¶„\n",
    "                for key, value in stats.items():\n",
    "                    if isinstance(value, float):\n",
    "                        print(f\"  {key}: {value:.2f}\")\n",
    "                    elif isinstance(value, int):\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                        \n",
    "            except Exception as plot_error:\n",
    "                print(f\"âš ï¸ ì‹œê°í™” ì—ëŸ¬: {plot_error}\")\n",
    "        \n",
    "        # ì •ê¸° ì €ì¥\n",
    "        if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
    "            try:\n",
    "                agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_checkpoint_{episode+1}.pth')\n",
    "            except Exception as checkpoint_error:\n",
    "                print(f\"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {checkpoint_error}\")\n",
    "    \n",
    "    # ìµœì¢… ì €ì¥\n",
    "    try:\n",
    "        agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_final.pth')\n",
    "        print(f\"âœ… {name} í•™ìŠµ ì™„ë£Œ!\")\n",
    "    except Exception as final_save_error:\n",
    "        print(f\"âš ï¸ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {final_save_error}\")\n",
    "        print(f\"âœ… {name} í•™ìŠµ ì™„ë£Œ! (ì €ì¥ ì‹¤íŒ¨)\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "print(\"ğŸ”§ í•™ìŠµ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c846c",
   "metadata": {},
   "source": [
    "## CNN ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN í•™ìŠµ ì‹¤í–‰\n",
    "cnn_monitor = train_agent(\n",
    "    agent=cnn_agent,\n",
    "    env=cnn_env,\n",
    "    monitor=cnn_monitor,\n",
    "    name=\"CNN\",\n",
    "    episodes=TRAINING_CONFIG['episodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46be8",
   "metadata": {},
   "source": [
    "## DNN ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26506513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN í•™ìŠµ ì‹¤í–‰\n",
    "dnn_monitor = train_agent(\n",
    "    agent=dnn_agent,\n",
    "    env=dnn_env,\n",
    "    monitor=dnn_monitor,\n",
    "    name=\"DNN\",\n",
    "    episodes=TRAINING_CONFIG['episodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191a35b",
   "metadata": {},
   "source": [
    "## ğŸ† ìµœì¢… ì„±ëŠ¥ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff302daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… í‰ê°€\n",
    "print(\"ğŸ† ìµœì¢… ì„±ëŠ¥ í‰ê°€ (ê° 50 ê²Œì„)\")\n",
    "\n",
    "cnn_final_score = evaluate_agent(cnn_agent, cnn_env, 50)\n",
    "dnn_final_score = evaluate_agent(dnn_agent, dnn_env, 50)\n",
    "\n",
    "print(f\"\\nğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
    "print(f\"  CNN í‰ê·  ì ìˆ˜: {cnn_final_score:.1f}\")\n",
    "print(f\"  DNN í‰ê·  ì ìˆ˜: {dnn_final_score:.1f}\")\n",
    "\n",
    "winner = \"CNN\" if cnn_final_score > dnn_final_score else \"DNN\"\n",
    "diff = abs(cnn_final_score - dnn_final_score)\n",
    "print(f\"\\nğŸ† ìŠ¹ì: {winner} (í‰ê·  {diff:.1f}ì  ì°¨ì´)\")\n",
    "\n",
    "# í†µê³„ ë¹„êµ\n",
    "cnn_stats = cnn_monitor.get_stats()\n",
    "dnn_stats = dnn_monitor.get_stats()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'CNN': [cnn_stats.get(k, 0) for k in ['avg_score', 'max_score', 'max_highest_tile', 'avg_steps']],\n",
    "    'DNN': [dnn_stats.get(k, 0) for k in ['avg_score', 'max_score', 'max_highest_tile', 'avg_steps']]\n",
    "}, index=['í‰ê·  ì ìˆ˜', 'ìµœê³  ì ìˆ˜', 'ìµœê³  íƒ€ì¼', 'í‰ê·  ìŠ¤í…'])\n",
    "\n",
    "print(\"\\nğŸ“ˆ ìƒì„¸ ë¹„êµ:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# ìµœì¢… ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# CNN ê²°ê³¼\n",
    "axes[0].plot(cnn_monitor.episode_scores, alpha=0.3, label='Episode Scores')\n",
    "axes[0].plot(pd.Series(cnn_monitor.episode_scores).rolling(100).mean(), label='Moving Average (100)')\n",
    "axes[0].set_title('CNN Training Progress')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "\n",
    "# DNN ê²°ê³¼\n",
    "axes[1].plot(dnn_monitor.episode_scores, alpha=0.3, label='Episode Scores')\n",
    "axes[1].plot(pd.Series(dnn_monitor.episode_scores).rolling(100).mean(), label='Moving Average (100)')\n",
    "axes[1].set_title('DNN Training Progress')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff8962",
   "metadata": {},
   "source": [
    "## ONNX ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë² ìŠ¤íŠ¸ ëª¨ë¸ ONNX ë³€í™˜\n",
    "print(\"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...\")\n",
    "\n",
    "# CNN ëª¨ë¸ ë³€í™˜\n",
    "cnn_agent.export_to_onnx(\n",
    "    filepath='/content/drive/MyDrive/2048_models/cnn_model.onnx',\n",
    "    input_shape=(4, 4, 16)\n",
    ")\n",
    "\n",
    "# DNN ëª¨ë¸ ë³€í™˜\n",
    "dnn_agent.export_to_onnx(\n",
    "    filepath='/content/drive/MyDrive/2048_models/dnn_model.onnx',\n",
    "    input_shape=(16,)\n",
    ")\n",
    "\n",
    "print(\"âœ… ONNX ë³€í™˜ ì™„ë£Œ!\")\n",
    "print(\"  - CNN: /content/drive/MyDrive/2048_models/cnn_model.onnx\")\n",
    "print(\"  - DNN: /content/drive/MyDrive/2048_models/dnn_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf49f2",
   "metadata": {},
   "source": [
    "## ë°ëª¨ í”Œë ˆì´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´ ë°ëª¨\n",
    "def demo_play(agent, env, name, render_interval=10):\n",
    "    print(f\"ğŸ® {name} ë°ëª¨ í”Œë ˆì´ ì‹œì‘!\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 1000:\n",
    "        if steps % render_interval == 0:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            print(f\"Step: {steps}\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "    env.render()\n",
    "    print(f\"\\nğŸ {name} ê²Œì„ ì¢…ë£Œ!\")\n",
    "    print(f\"  ìµœì¢… ì ìˆ˜: {info['score']}\")\n",
    "    print(f\"  ìµœê³  íƒ€ì¼: {info['highest']}\")\n",
    "    print(f\"  ì´ ìŠ¤í…: {steps}\")\n",
    "\n",
    "# ë² ìŠ¤íŠ¸ ëª¨ë¸ë¡œ ë°ëª¨\n",
    "best_agent = cnn_agent if cnn_final_score > dnn_final_score else dnn_agent\n",
    "best_env = cnn_env if cnn_final_score > dnn_final_score else dnn_env\n",
    "best_name = \"CNN\" if cnn_final_score > dnn_final_score else \"DNN\"\n",
    "\n",
    "demo_play(best_agent, best_env, f\"Best {best_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
