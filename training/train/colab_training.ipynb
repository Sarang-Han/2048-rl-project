{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70268a27",
      "metadata": {
        "id": "70268a27"
      },
      "source": [
        "# 2048 DQN 모델 학습\n",
        "\n",
        "## 학습 계획\n",
        "- **목적**: 2048 게임에서 고득점을 달성하는 CNN 기반 DQN 에이전트 학습\n",
        "- **아키텍처**: CNN (Layered)\n",
        "- **환경**: Google Colab GPU 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0f131c",
      "metadata": {
        "id": "cd0f131c"
      },
      "source": [
        "## 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea4d482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ea4d482",
        "outputId": "53deeb4c-75d6-41a9-ce45-fbeb1e371c40"
      },
      "outputs": [],
      "source": [
        "# 필요한 패키지 설치\n",
        "!pip install torch torchvision gym matplotlib seaborn tensorboard\n",
        "!pip install onnx onnxruntime\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch 버전: {torch.__version__}\")\n",
        "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2aa566b",
      "metadata": {
        "id": "a2aa566b"
      },
      "source": [
        "## 코드 업로드 및 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e40528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e40528",
        "outputId": "f61d4581-0161-4846-a3bc-4f5bd3c284c4"
      },
      "outputs": [],
      "source": [
        "# Google Drive 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 프로젝트 경로 설정\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/2048-rl-project'\n",
        "TRAINING_PATH = os.path.join(PROJECT_ROOT, 'training')\n",
        "\n",
        "# Python 경로에 추가\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "if TRAINING_PATH not in sys.path:\n",
        "    sys.path.insert(0, TRAINING_PATH)\n",
        "\n",
        "# 작업 디렉토리 변경\n",
        "os.chdir(TRAINING_PATH)\n",
        "\n",
        "print(f\"  경로 설정 완료!\")\n",
        "print(f\"  프로젝트 루트: {PROJECT_ROOT}\")\n",
        "print(f\"  학습 경로: {TRAINING_PATH}\")\n",
        "print(f\"  현재 작업 디렉토리: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1d7c58",
      "metadata": {
        "id": "1a1d7c58"
      },
      "source": [
        "## 모델 및 환경 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d975255d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d975255d",
        "outputId": "cde93c47-5b7e-425e-c783-e662a3e3521e"
      },
      "outputs": [],
      "source": [
        "# 모듈 import 및 테스트\n",
        "print(\"모듈 로딩 시작...\")\n",
        "\n",
        "from environment.game_2048 import Game2048Env\n",
        "from models.dqn_agent import DQNAgent\n",
        "from models.networks import count_parameters\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "print(\"모듈 로드 완료\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1de60ca",
      "metadata": {
        "id": "c1de60ca"
      },
      "source": [
        "## 학습 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb624383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb624383",
        "outputId": "42e20213-5ba7-49ab-8872-9a03fc8e93de"
      },
      "outputs": [],
      "source": [
        "TRAINING_CONFIG = {\n",
        "    # 기본 설정\n",
        "    'episodes': 3000,\n",
        "    'max_steps_per_episode': 5000,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # DQN 설정\n",
        "    'buffer_size': 100000,\n",
        "    'batch_size': 64,\n",
        "    'lr': 1e-4,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 0.9,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 30000,\n",
        "    'target_update': 1000,\n",
        "\n",
        "    # 평가 및 로깅 설정\n",
        "    'eval_interval': 100,\n",
        "    'eval_episodes': 10,\n",
        "    'save_interval': 500,\n",
        "    'plot_interval': 50,\n",
        "    'log_interval': 10\n",
        "}\n",
        "\n",
        "print(\" 학습 설정 완료:\")\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c94975f",
      "metadata": {
        "id": "9c94975f"
      },
      "source": [
        "## 모델 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fa33dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17fa33dd",
        "outputId": "3d636bab-2dd4-4cec-bf75-01c551263cef"
      },
      "outputs": [],
      "source": [
        "# 에이전트 생성 함수\n",
        "def create_agent_and_env():\n",
        "    \"\"\"에이전트와 환경 생성\"\"\"\n",
        "    env = Game2048Env()\n",
        "\n",
        "    agent = DQNAgent(\n",
        "        lr=TRAINING_CONFIG['lr'],\n",
        "        gamma=TRAINING_CONFIG['gamma'],\n",
        "        epsilon_start=TRAINING_CONFIG['epsilon_start'],\n",
        "        epsilon_end=TRAINING_CONFIG['epsilon_end'],\n",
        "        epsilon_decay=TRAINING_CONFIG['epsilon_decay'],\n",
        "        buffer_size=TRAINING_CONFIG['buffer_size'],\n",
        "        batch_size=TRAINING_CONFIG['batch_size'],\n",
        "        target_update=TRAINING_CONFIG['target_update'],\n",
        "        double_dqn=True,           # Double DQN 활성화\n",
        "        dueling=True,             # dueling head 활성화 여부\n",
        "        prioritized_replay=True,   # 성능 향상\n",
        "        device=TRAINING_CONFIG['device']\n",
        "    )\n",
        "\n",
        "    return agent, env\n",
        "\n",
        "print(\" CNN 에이전트 생성...\")\n",
        "agent, env = create_agent_and_env()\n",
        "print(f\" 모델 파라미터: {count_parameters(agent.q_network):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e08891f",
      "metadata": {
        "id": "9e08891f"
      },
      "source": [
        "## 학습 모니터링 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6faa7ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6faa7ae",
        "outputId": "5221f46c-21f0-41d2-f2b1-ddf8bacb508e"
      },
      "outputs": [],
      "source": [
        "class TrainingMonitor:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.episode_rewards = []\n",
        "        self.episode_scores = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_losses = []\n",
        "        self.highest_tiles = []\n",
        "        self.eval_scores = []\n",
        "        self.eval_episodes = []\n",
        "\n",
        "    def add_episode(self, reward, score, steps, loss, highest_tile):\n",
        "        self.episode_rewards.append(reward)\n",
        "        self.episode_scores.append(score)\n",
        "        self.episode_steps.append(steps)\n",
        "        self.episode_losses.append(loss)\n",
        "        self.highest_tiles.append(highest_tile)\n",
        "\n",
        "    def add_eval(self, episode, avg_score):\n",
        "        self.eval_episodes.append(episode)\n",
        "        self.eval_scores.append(avg_score)\n",
        "\n",
        "    def plot_progress(self, title=\"Training Progress\"):\n",
        "        if len(self.episode_rewards) < 10:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "        # 보상\n",
        "        axes[0,0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
        "        axes[0,0].plot(pd.Series(self.episode_rewards).rolling(50).mean(), color='red')\n",
        "        axes[0,0].set_title('Episode Rewards')\n",
        "        axes[0,0].set_ylabel('Reward')\n",
        "\n",
        "        # 점수\n",
        "        axes[0,1].plot(self.episode_scores, alpha=0.3, color='green')\n",
        "        axes[0,1].plot(pd.Series(self.episode_scores).rolling(50).mean(), color='red')\n",
        "        axes[0,1].set_title('Episode Scores')\n",
        "        axes[0,1].set_ylabel('Score')\n",
        "\n",
        "        # 최고 타일\n",
        "        axes[0,2].plot(self.highest_tiles, alpha=0.3, color='purple')\n",
        "        axes[0,2].plot(pd.Series(self.highest_tiles).rolling(50).mean(), color='red')\n",
        "        axes[0,2].set_title('Highest Tiles')\n",
        "        axes[0,2].set_ylabel('Tile Value')\n",
        "\n",
        "        # 스텝 수\n",
        "        axes[1,0].plot(self.episode_steps, alpha=0.3, color='orange')\n",
        "        axes[1,0].plot(pd.Series(self.episode_steps).rolling(50).mean(), color='red')\n",
        "        axes[1,0].set_title('Episode Steps')\n",
        "        axes[1,0].set_ylabel('Steps')\n",
        "        axes[1,0].set_xlabel('Episode')\n",
        "\n",
        "        # 손실\n",
        "        if self.episode_losses and any(loss is not None for loss in self.episode_losses):\n",
        "            valid_losses = [l for l in self.episode_losses if l is not None]\n",
        "            if valid_losses:\n",
        "                axes[1,1].plot(valid_losses, alpha=0.3, color='red')\n",
        "                axes[1,1].plot(pd.Series(valid_losses).rolling(20).mean(), color='darkred')\n",
        "        axes[1,1].set_title('Training Loss')\n",
        "        axes[1,1].set_ylabel('Loss')\n",
        "        axes[1,1].set_xlabel('Episode')\n",
        "\n",
        "        # 평가 점수\n",
        "        if self.eval_scores:\n",
        "            axes[1,2].plot(self.eval_episodes, self.eval_scores, 'o-', color='darkgreen')\n",
        "        axes[1,2].set_title('Evaluation Scores')\n",
        "        axes[1,2].set_ylabel('Avg Score')\n",
        "        axes[1,2].set_xlabel('Episode')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.episode_scores:\n",
        "            return {}\n",
        "\n",
        "        recent_scores = self.episode_scores[-100:]\n",
        "        recent_tiles = self.highest_tiles[-100:]\n",
        "\n",
        "        return {\n",
        "            'episodes': len(self.episode_scores),\n",
        "            'avg_score': np.mean(recent_scores),\n",
        "            'max_score': max(self.episode_scores),\n",
        "            'avg_highest_tile': np.mean(recent_tiles),\n",
        "            'max_highest_tile': max(self.highest_tiles),\n",
        "            'avg_steps': np.mean(self.episode_steps[-100:])\n",
        "        }\n",
        "\n",
        "monitor = TrainingMonitor()\n",
        "print(\" 학습 모니터링 시스템 준비 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d8e403",
      "metadata": {
        "id": "e3d8e403"
      },
      "source": [
        "## 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1a6fb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1a6fb7",
        "outputId": "c43b669a-0e97-46a5-b214-b885dee5183e"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(agent, env, num_episodes=5):\n",
        "    \"\"\"액션 마스킹이 적용된 에이전트 평가 함수\"\"\"\n",
        "    total_scores = []\n",
        "    total_steps = []\n",
        "    highest_tiles = []\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        steps = 0\n",
        "        while steps < 1000:\n",
        "            # 액션 마스킹 적용\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            if not valid_actions:  # 게임 종료 (안전장치)\n",
        "                break\n",
        "                \n",
        "            action = agent.select_action(state, training=False, valid_actions=valid_actions)\n",
        "            next_state, _, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                total_scores.append(info['score'])\n",
        "                total_steps.append(steps)\n",
        "                highest_tiles.append(info['highest'])\n",
        "                break\n",
        "    \n",
        "    return {\n",
        "        'avg_score': np.mean(total_scores) if total_scores else 0,\n",
        "        'avg_steps': np.mean(total_steps) if total_steps else 0,\n",
        "        'avg_highest': np.mean(highest_tiles) if highest_tiles else 0,\n",
        "        'max_score': max(total_scores) if total_scores else 0,\n",
        "        'max_highest': max(highest_tiles) if highest_tiles else 0\n",
        "    }\n",
        "\n",
        "def train_agent(agent, env, monitor, episodes):\n",
        "    \"\"\"액션 마스킹이 적용된 에이전트 학습 함수 (수정된 버전)\"\"\"\n",
        "    print(f\"액션 마스킹 적용 CNN 모델 학습 시작! (목표: {episodes} 에피소드)\")\n",
        "    start_time = time.time()\n",
        "    best_score = 0\n",
        "    best_highest = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        episode_losses = []\n",
        "        final_info = {'score': 0, 'highest': 0, 'valid_actions': []}  # 기본값 설정\n",
        "\n",
        "        while steps < TRAINING_CONFIG['max_steps_per_episode']:\n",
        "            # 액션 마스킹 적용\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            \n",
        "            # 유효한 액션이 없으면 게임 종료\n",
        "            if not valid_actions:\n",
        "                print(f\"Episode {episode+1}: No valid actions available at step {steps}\")\n",
        "                break\n",
        "            \n",
        "            # 액션 선택 (유효한 액션만 고려)\n",
        "            action = agent.select_action(state, training=True, valid_actions=valid_actions)\n",
        "            \n",
        "            # 환경에서 액션 실행\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            final_info = info  # 마지막 info 저장\n",
        "            \n",
        "            # 경험 저장\n",
        "            if hasattr(agent, 'prioritized_replay') and agent.prioritized_replay and agent.memory.is_ready(agent.batch_size):\n",
        "                # TD error 미리 계산\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
        "                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(agent.device)\n",
        "                    current_q = agent.q_network(state_tensor)[0][action]\n",
        "                    next_q = agent.target_network(next_state_tensor).max(1)[0]\n",
        "                    target_q = reward + (agent.gamma * next_q * (not done))\n",
        "                    td_error = abs((target_q - current_q).item())\n",
        "                    agent.store_experience(state, action, reward, next_state, done, td_error)\n",
        "            else:\n",
        "                agent.store_experience(state, action, reward, next_state, done)\n",
        "\n",
        "            # 학습 수행\n",
        "            if agent.memory.is_ready(agent.batch_size):\n",
        "                loss = agent.train_step()\n",
        "                if loss is not None:\n",
        "                    episode_losses.append(loss)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 에피소드 통계 업데이트\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else None\n",
        "        monitor.add_episode(total_reward, final_info['score'], steps, avg_loss, final_info['highest'])\n",
        "        agent.episode_rewards.append(total_reward)\n",
        "\n",
        "        # 최고 기록 갱신\n",
        "        if final_info['score'] > best_score:\n",
        "            best_score = final_info['score']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_score.pth')\n",
        "            \n",
        "        if final_info['highest'] > best_highest:\n",
        "            best_highest = final_info['highest']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_tile.pth')\n",
        "\n",
        "        # 주기적 로깅\n",
        "        if (episode + 1) % TRAINING_CONFIG['log_interval'] == 0:\n",
        "            stats = agent.get_stats()\n",
        "            elapsed = time.time() - start_time\n",
        "            loss_str = f\"{avg_loss:.4f}\" if avg_loss is not None else \"N/A\"\n",
        "            \n",
        "            print(f\"Episode {episode+1:4d} | Score: {final_info['score']:5.0f} | Highest: {final_info['highest']:4.0f} | \"\n",
        "                  f\"Reward: {total_reward:7.2f} | Steps: {steps:3d} | Valid: {len(final_info['valid_actions'])} | \"\n",
        "                  f\"ε: {stats['epsilon']:.3f} | Loss: {loss_str} | Time: {elapsed/60:.1f}min\")\n",
        "            \n",
        "        if (episode + 1) % TRAINING_CONFIG['eval_interval'] == 0:\n",
        "            eval_results = evaluate_agent(agent, env, TRAINING_CONFIG['eval_episodes'])\n",
        "            monitor.add_eval(episode + 1, eval_results['avg_score'])\n",
        "            \n",
        "            print(f\"🎯 평가 결과 (Episode {episode+1}):\")\n",
        "            print(f\"   평균 점수: {eval_results['avg_score']:.1f}\")\n",
        "            print(f\"   최고 점수: {eval_results['max_score']:.0f}\")\n",
        "            print(f\"   평균 최고타일: {eval_results['avg_highest']:.0f}\")\n",
        "            print(f\"   최고 타일: {eval_results['max_highest']:.0f}\")\n",
        "\n",
        "        # 주기적 시각화\n",
        "        if (episode + 1) % TRAINING_CONFIG['plot_interval'] == 0:\n",
        "            clear_output(wait=True)\n",
        "            monitor.plot_progress(f\"CNN Training Progress - Episode {episode+1}\")\n",
        "            \n",
        "            stats = monitor.get_stats()\n",
        "            print(f\" 현재 통계 (최근 100 에피소드):\")\n",
        "            for key, value in stats.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.2f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "            \n",
        "            print(f\"  현재 epsilon: {agent.get_epsilon():.4f}\")\n",
        "            print(f\"  메모리 사용량: {len(agent.memory):,}/{agent.memory.capacity:,}\")\n",
        "\n",
        "        # 주기적 모델 저장\n",
        "        if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_checkpoint_{episode+1}.pth')\n",
        "\n",
        "    # 최종 모델 저장\n",
        "    agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_final_action_masked.pth')\n",
        "    \n",
        "    print(f\"\\n 액션 마스킹 적용 학습 완료!\")\n",
        "    print(f\" 최고 점수: {best_score}\")\n",
        "    print(f\" 최고 타일: {best_highest}\")\n",
        "    \n",
        "    return monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35c846c",
      "metadata": {
        "id": "f35c846c"
      },
      "source": [
        "## CNN 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bf4881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "73bf4881",
        "outputId": "015c3dff-d775-45d5-cd1a-7f201d50c56b"
      },
      "outputs": [],
      "source": [
        "monitor = train_agent(\n",
        "    agent=agent,\n",
        "    env=env,\n",
        "    monitor=monitor,\n",
        "    episodes=TRAINING_CONFIG['episodes']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0191a35b",
      "metadata": {
        "id": "0191a35b"
      },
      "source": [
        "## 최종 성능 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47bca33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 최종 모델 성능 평가 및 시각화\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class ModelPerformanceEvaluator:\n",
        "    def __init__(self, agent, env):\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        \n",
        "    def evaluate_comprehensive(self, num_games=100):\n",
        "        \"\"\"포괄적인 성능 평가\"\"\"\n",
        "        print(f\"Final Model Evaluation Started ({num_games} games)\")\n",
        "        \n",
        "        results = {\n",
        "            'scores': [],\n",
        "            'highest_tiles': [],\n",
        "            'steps': [],\n",
        "            'game_details': []  # 최고/최악 게임 분석용\n",
        "        }\n",
        "        \n",
        "        for game_idx in range(num_games):\n",
        "            game_result = self._play_single_game()\n",
        "            \n",
        "            results['scores'].append(game_result['final_score'])\n",
        "            results['highest_tiles'].append(game_result['highest_tile'])\n",
        "            results['steps'].append(game_result['steps'])\n",
        "            results['game_details'].append(game_result)\n",
        "            \n",
        "            if (game_idx + 1) % 25 == 0:\n",
        "                print(f\"  Progress: {game_idx + 1}/{num_games} games completed\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _play_single_game(self):\n",
        "        \"\"\"단일 게임 플레이\"\"\"\n",
        "        state = self.env.reset()\n",
        "        steps = 0\n",
        "        \n",
        "        while steps < 2000:\n",
        "            valid_actions = self.env.get_valid_actions()\n",
        "            if not valid_actions:\n",
        "                break\n",
        "                \n",
        "            action = self.agent.select_action(state, training=False, valid_actions=valid_actions)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        return {\n",
        "            'final_score': info['score'],\n",
        "            'highest_tile': info['highest'],\n",
        "            'steps': steps,\n",
        "            'final_board': self.env.get_board().copy()\n",
        "        }\n",
        "    \n",
        "    def analyze_and_visualize(self, results):\n",
        "        \"\"\"핵심 분석 및 시각화\"\"\"\n",
        "        scores = np.array(results['scores'])\n",
        "        highest_tiles = np.array(results['highest_tiles'])\n",
        "        \n",
        "        # 1. 타일별 달성률 분석\n",
        "        tile_achievements = Counter(highest_tiles)\n",
        "        print(\"\\n Tile Achievement Rates:\")\n",
        "        total_games = len(scores)\n",
        "        for tile in sorted(tile_achievements.keys(), reverse=True):\n",
        "            count = tile_achievements[tile]\n",
        "            percentage = (count / total_games) * 100\n",
        "            print(f\"  {tile:4d} tile: {count:3d} games ({percentage:5.1f}%)\")\n",
        "        \n",
        "        # 2. 최고/최악 게임 찾기\n",
        "        best_idx = np.argmax(scores)\n",
        "        worst_idx = np.argmin(scores)\n",
        "        \n",
        "        best_game = results['game_details'][best_idx]\n",
        "        worst_game = results['game_details'][worst_idx]\n",
        "        \n",
        "        # 3. 시각화\n",
        "        self._plot_analysis(tile_achievements, best_game, worst_game, scores)\n",
        "        \n",
        "        # 4. 요약 통계\n",
        "        print(f\"\\n Performance Summary:\")\n",
        "        print(f\"  Average Score: {np.mean(scores):.1f} ± {np.std(scores):.1f}\")\n",
        "        print(f\"  Best Score: {np.max(scores):.0f} (Highest Tile: {best_game['highest_tile']})\")\n",
        "        print(f\"  Worst Score: {np.min(scores):.0f} (Highest Tile: {worst_game['highest_tile']})\")\n",
        "        print(f\"  Average Highest Tile: {np.mean(highest_tiles):.1f}\")\n",
        "    \n",
        "    def _plot_analysis(self, tile_achievements, best_game, worst_game, scores):\n",
        "        \"\"\"핵심 시각화\"\"\"\n",
        "        # 3개 서브플롯 크기 동일하게 설정\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        \n",
        "        # 1. 타일 달성률\n",
        "        plt.subplot(1, 3, 1)\n",
        "        tiles = sorted(tile_achievements.keys(), reverse=True)\n",
        "        counts = [tile_achievements[tile] for tile in tiles]\n",
        "        total_games = sum(counts)\n",
        "        percentages = [(count / total_games) * 100 for count in counts]\n",
        "        \n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(tiles)))\n",
        "        bars = plt.bar(range(len(tiles)), percentages, color=colors)\n",
        "        plt.title('Highest Tile Achievement Rate')\n",
        "        plt.xlabel('Highest Tile')\n",
        "        plt.ylabel('Achievement Rate (%)')\n",
        "        plt.xticks(range(len(tiles)), [str(tile) for tile in tiles], rotation=45)\n",
        "        \n",
        "        # 비율 표시\n",
        "        for i, (bar, count, pct) in enumerate(zip(bars, counts, percentages)):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5, \n",
        "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # 2. 최고 게임 보드\n",
        "        plt.subplot(1, 3, 2)\n",
        "        self._plot_board(best_game['final_board'], \"Best Game\")\n",
        "        # 점수와 최고 타일 정보를 살짝 올려서 표시\n",
        "        plt.text(0.5, -0.10, f\"Score: {best_game['final_score']}\\nHighest: {best_game['highest_tile']}\", \n",
        "                ha='center', va='top', transform=plt.gca().transAxes, fontsize=10, color='black')\n",
        "        \n",
        "        # 3. 최악 게임 보드\n",
        "        plt.subplot(1, 3, 3)\n",
        "        self._plot_board(worst_game['final_board'], \"Worst Game\")\n",
        "        # 점수와 최고 타일 정보를 살짝 올려서 표시\n",
        "        plt.text(0.5, -0.10, f\"Score: {worst_game['final_score']}\\nHighest: {worst_game['highest_tile']}\", \n",
        "                ha='center', va='top', transform=plt.gca().transAxes, fontsize=10, color='black')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def _get_tile_color(self, tile_value):\n",
        "        \"\"\"2048 게임 전통 색상 매핑\"\"\"\n",
        "        color_map = {\n",
        "            0: '#CDC1B4',    # 빈 공간 (회색)\n",
        "            2: '#EEE4DA',    # Light yellow/beige\n",
        "            4: '#EDE0C8',    # Light orange/yellow\n",
        "            8: '#F2B179',    # Darker orange\n",
        "            16: '#F59563',   # Orange\n",
        "            32: '#F67C5F',   # Red-orange\n",
        "            64: '#F65E3B',   # Red\n",
        "            128: '#EDCF72',  # Orange/red shade (237, 207, 114)\n",
        "            256: '#EDCC61',  # Orange/red shade (237, 204, 97)\n",
        "            512: '#EDC850',  # Orange/red shade (237, 200, 80)\n",
        "            1024: '#EDC53F', # Orange/red shade (237, 197, 63)\n",
        "            2048: '#EDC22E', # Orange/red shade (237, 194, 46)\n",
        "            4096: '#000000', # Black\n",
        "        }\n",
        "        \n",
        "        # 정의된 색상이 없으면 가장 높은 색상 사용\n",
        "        if tile_value in color_map:\n",
        "            return color_map[tile_value]\n",
        "        elif tile_value > 2048:\n",
        "            return color_map[4096]\n",
        "        else:\n",
        "            return color_map[0]\n",
        "    \n",
        "    def _plot_board(self, board, title):\n",
        "        \"\"\"보드 시각화 (타일별 고정 색상)\"\"\"\n",
        "        # 4x4 색상 배열 생성\n",
        "        color_board = np.zeros((4, 4, 3))\n",
        "        \n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                tile_value = int(board[i, j])\n",
        "                hex_color = self._get_tile_color(tile_value)\n",
        "                # hex를 RGB로 변환\n",
        "                rgb = tuple(int(hex_color[k:k+2], 16)/255.0 for k in (1, 3, 5))\n",
        "                color_board[i, j] = rgb\n",
        "        \n",
        "        plt.imshow(color_board, aspect='equal')\n",
        "        plt.title(title, fontsize=12, pad=15)  # 패딩 추가\n",
        "        \n",
        "        # 각 셀에 실제 값 표시 (폰트 크기 증가)\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                tile_value = int(board[i, j])\n",
        "                if tile_value > 0:\n",
        "                    # 텍스트 색상 결정 (타일 색상에 따라)\n",
        "                    text_color = 'white' if tile_value >= 8 else 'black'\n",
        "                    # 폰트 크기 조정 (기존보다 2pt 증가)\n",
        "                    font_size = 14 if tile_value < 1000 else 12\n",
        "                    \n",
        "                    plt.text(j, i, str(tile_value), ha='center', va='center', \n",
        "                           fontsize=font_size, fontweight='bold', color=text_color)\n",
        "        \n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "# ======================== 평가 실행 ========================\n",
        "\n",
        "print(\" 최종 모델 성능 평가 시작...\")\n",
        "evaluator = ModelPerformanceEvaluator(agent, env)\n",
        "\n",
        "# 100게임 평가\n",
        "results = evaluator.evaluate_comprehensive(num_games=100)\n",
        "\n",
        "# 분석 및 시각화\n",
        "evaluator.analyze_and_visualize(results)\n",
        "\n",
        "print(\"\\n 모델 평가 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fff8962",
      "metadata": {
        "id": "0fff8962"
      },
      "source": [
        "## ONNX 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0b97a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0b97a7",
        "outputId": "12ccaca5-9101-418c-d0d5-4675fa248d0d"
      },
      "outputs": [],
      "source": [
        "print(\" ONNX 변환 시작...\")\n",
        "agent.export_to_onnx(\n",
        "    filepath='/content/drive/MyDrive/2048_models/cnn_model.onnx',\n",
        "    input_shape=(4, 4, 16)\n",
        ")\n",
        "print(\" ONNX 변환 완료!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
