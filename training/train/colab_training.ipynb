{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70268a27",
      "metadata": {
        "id": "70268a27"
      },
      "source": [
        "# 2048 DQN ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "## í•™ìŠµ ê³„íš\n",
        "- **ëª©ì **: 2048 ê²Œì„ì—ì„œ ê³ ë“ì ì„ ë‹¬ì„±í•˜ëŠ” CNN ê¸°ë°˜ DQN ì—ì´ì „íŠ¸ í•™ìŠµ\n",
        "- **ì•„í‚¤í…ì²˜**: CNN (Layered)\n",
        "- **í™˜ê²½**: Google Colab GPU ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0f131c",
      "metadata": {
        "id": "cd0f131c"
      },
      "source": [
        "## í™˜ê²½ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea4d482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ea4d482",
        "outputId": "53deeb4c-75d6-41a9-ce45-fbeb1e371c40"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install torch torchvision gym matplotlib seaborn tensorboard\n",
        "!pip install onnx onnxruntime\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
        "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2aa566b",
      "metadata": {
        "id": "a2aa566b"
      },
      "source": [
        "## ì½”ë“œ ì—…ë¡œë“œ ë° ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e40528",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93e40528",
        "outputId": "f61d4581-0161-4846-a3bc-4f5bd3c284c4"
      },
      "outputs": [],
      "source": [
        "# Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/2048-rl-project'\n",
        "TRAINING_PATH = os.path.join(PROJECT_ROOT, 'training')\n",
        "\n",
        "# Python ê²½ë¡œì— ì¶”ê°€\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "if TRAINING_PATH not in sys.path:\n",
        "    sys.path.insert(0, TRAINING_PATH)\n",
        "\n",
        "# ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½\n",
        "os.chdir(TRAINING_PATH)\n",
        "\n",
        "print(f\"  ê²½ë¡œ ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"  í”„ë¡œì íŠ¸ ë£¨íŠ¸: {PROJECT_ROOT}\")\n",
        "print(f\"  í•™ìŠµ ê²½ë¡œ: {TRAINING_PATH}\")\n",
        "print(f\"  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a1d7c58",
      "metadata": {
        "id": "1a1d7c58"
      },
      "source": [
        "## ëª¨ë¸ ë° í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d975255d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d975255d",
        "outputId": "cde93c47-5b7e-425e-c783-e662a3e3521e"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë“ˆ import ë° í…ŒìŠ¤íŠ¸\n",
        "print(\"ëª¨ë“ˆ ë¡œë”© ì‹œì‘...\")\n",
        "\n",
        "from environment.game_2048 import Game2048Env\n",
        "from models.dqn_agent import DQNAgent\n",
        "from models.networks import count_parameters\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "\n",
        "print(\"ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1de60ca",
      "metadata": {
        "id": "c1de60ca"
      },
      "source": [
        "## í•™ìŠµ ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb624383",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb624383",
        "outputId": "42e20213-5ba7-49ab-8872-9a03fc8e93de"
      },
      "outputs": [],
      "source": [
        "TRAINING_CONFIG = {\n",
        "    # ê¸°ë³¸ ì„¤ì •\n",
        "    'episodes': 3000,\n",
        "    'max_steps_per_episode': 5000,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # DQN ì„¤ì •\n",
        "    'buffer_size': 100000,\n",
        "    'batch_size': 64,\n",
        "    'lr': 1e-4,\n",
        "    'gamma': 0.99,\n",
        "    'epsilon_start': 0.9,\n",
        "    'epsilon_end': 0.01,\n",
        "    'epsilon_decay': 30000,\n",
        "    'target_update': 1000,\n",
        "\n",
        "    # í‰ê°€ ë° ë¡œê¹… ì„¤ì •\n",
        "    'eval_interval': 100,\n",
        "    'eval_episodes': 10,\n",
        "    'save_interval': 500,\n",
        "    'plot_interval': 50,\n",
        "    'log_interval': 10\n",
        "}\n",
        "\n",
        "print(\" í•™ìŠµ ì„¤ì • ì™„ë£Œ:\")\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c94975f",
      "metadata": {
        "id": "9c94975f"
      },
      "source": [
        "## ëª¨ë¸ ì´ˆê¸°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fa33dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17fa33dd",
        "outputId": "3d636bab-2dd4-4cec-bf75-01c551263cef"
      },
      "outputs": [],
      "source": [
        "# ì—ì´ì „íŠ¸ ìƒì„± í•¨ìˆ˜\n",
        "def create_agent_and_env():\n",
        "    \"\"\"ì—ì´ì „íŠ¸ì™€ í™˜ê²½ ìƒì„±\"\"\"\n",
        "    env = Game2048Env()\n",
        "\n",
        "    agent = DQNAgent(\n",
        "        lr=TRAINING_CONFIG['lr'],\n",
        "        gamma=TRAINING_CONFIG['gamma'],\n",
        "        epsilon_start=TRAINING_CONFIG['epsilon_start'],\n",
        "        epsilon_end=TRAINING_CONFIG['epsilon_end'],\n",
        "        epsilon_decay=TRAINING_CONFIG['epsilon_decay'],\n",
        "        buffer_size=TRAINING_CONFIG['buffer_size'],\n",
        "        batch_size=TRAINING_CONFIG['batch_size'],\n",
        "        target_update=TRAINING_CONFIG['target_update'],\n",
        "        double_dqn=True,           # Double DQN í™œì„±í™”\n",
        "        dueling=True,             # dueling head í™œì„±í™” ì—¬ë¶€\n",
        "        prioritized_replay=True,   # ì„±ëŠ¥ í–¥ìƒ\n",
        "        device=TRAINING_CONFIG['device']\n",
        "    )\n",
        "\n",
        "    return agent, env\n",
        "\n",
        "print(\" CNN ì—ì´ì „íŠ¸ ìƒì„±...\")\n",
        "agent, env = create_agent_and_env()\n",
        "print(f\" ëª¨ë¸ íŒŒë¼ë¯¸í„°: {count_parameters(agent.q_network):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e08891f",
      "metadata": {
        "id": "9e08891f"
      },
      "source": [
        "## í•™ìŠµ ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6faa7ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6faa7ae",
        "outputId": "5221f46c-21f0-41d2-f2b1-ddf8bacb508e"
      },
      "outputs": [],
      "source": [
        "class TrainingMonitor:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.episode_rewards = []\n",
        "        self.episode_scores = []\n",
        "        self.episode_steps = []\n",
        "        self.episode_losses = []\n",
        "        self.highest_tiles = []\n",
        "        self.eval_scores = []\n",
        "        self.eval_episodes = []\n",
        "\n",
        "    def add_episode(self, reward, score, steps, loss, highest_tile):\n",
        "        self.episode_rewards.append(reward)\n",
        "        self.episode_scores.append(score)\n",
        "        self.episode_steps.append(steps)\n",
        "        self.episode_losses.append(loss)\n",
        "        self.highest_tiles.append(highest_tile)\n",
        "\n",
        "    def add_eval(self, episode, avg_score):\n",
        "        self.eval_episodes.append(episode)\n",
        "        self.eval_scores.append(avg_score)\n",
        "\n",
        "    def plot_progress(self, title=\"Training Progress\"):\n",
        "        if len(self.episode_rewards) < 10:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "        # ë³´ìƒ\n",
        "        axes[0,0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
        "        axes[0,0].plot(pd.Series(self.episode_rewards).rolling(50).mean(), color='red')\n",
        "        axes[0,0].set_title('Episode Rewards')\n",
        "        axes[0,0].set_ylabel('Reward')\n",
        "\n",
        "        # ì ìˆ˜\n",
        "        axes[0,1].plot(self.episode_scores, alpha=0.3, color='green')\n",
        "        axes[0,1].plot(pd.Series(self.episode_scores).rolling(50).mean(), color='red')\n",
        "        axes[0,1].set_title('Episode Scores')\n",
        "        axes[0,1].set_ylabel('Score')\n",
        "\n",
        "        # ìµœê³  íƒ€ì¼\n",
        "        axes[0,2].plot(self.highest_tiles, alpha=0.3, color='purple')\n",
        "        axes[0,2].plot(pd.Series(self.highest_tiles).rolling(50).mean(), color='red')\n",
        "        axes[0,2].set_title('Highest Tiles')\n",
        "        axes[0,2].set_ylabel('Tile Value')\n",
        "\n",
        "        # ìŠ¤í… ìˆ˜\n",
        "        axes[1,0].plot(self.episode_steps, alpha=0.3, color='orange')\n",
        "        axes[1,0].plot(pd.Series(self.episode_steps).rolling(50).mean(), color='red')\n",
        "        axes[1,0].set_title('Episode Steps')\n",
        "        axes[1,0].set_ylabel('Steps')\n",
        "        axes[1,0].set_xlabel('Episode')\n",
        "\n",
        "        # ì†ì‹¤\n",
        "        if self.episode_losses and any(loss is not None for loss in self.episode_losses):\n",
        "            valid_losses = [l for l in self.episode_losses if l is not None]\n",
        "            if valid_losses:\n",
        "                axes[1,1].plot(valid_losses, alpha=0.3, color='red')\n",
        "                axes[1,1].plot(pd.Series(valid_losses).rolling(20).mean(), color='darkred')\n",
        "        axes[1,1].set_title('Training Loss')\n",
        "        axes[1,1].set_ylabel('Loss')\n",
        "        axes[1,1].set_xlabel('Episode')\n",
        "\n",
        "        # í‰ê°€ ì ìˆ˜\n",
        "        if self.eval_scores:\n",
        "            axes[1,2].plot(self.eval_episodes, self.eval_scores, 'o-', color='darkgreen')\n",
        "        axes[1,2].set_title('Evaluation Scores')\n",
        "        axes[1,2].set_ylabel('Avg Score')\n",
        "        axes[1,2].set_xlabel('Episode')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_stats(self):\n",
        "        if not self.episode_scores:\n",
        "            return {}\n",
        "\n",
        "        recent_scores = self.episode_scores[-100:]\n",
        "        recent_tiles = self.highest_tiles[-100:]\n",
        "\n",
        "        return {\n",
        "            'episodes': len(self.episode_scores),\n",
        "            'avg_score': np.mean(recent_scores),\n",
        "            'max_score': max(self.episode_scores),\n",
        "            'avg_highest_tile': np.mean(recent_tiles),\n",
        "            'max_highest_tile': max(self.highest_tiles),\n",
        "            'avg_steps': np.mean(self.episode_steps[-100:])\n",
        "        }\n",
        "\n",
        "monitor = TrainingMonitor()\n",
        "print(\" í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d8e403",
      "metadata": {
        "id": "e3d8e403"
      },
      "source": [
        "## í•™ìŠµ í•¨ìˆ˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1a6fb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a1a6fb7",
        "outputId": "c43b669a-0e97-46a5-b214-b885dee5183e"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(agent, env, num_episodes=5):\n",
        "    \"\"\"ì•¡ì…˜ ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ í‰ê°€ í•¨ìˆ˜\"\"\"\n",
        "    total_scores = []\n",
        "    total_steps = []\n",
        "    highest_tiles = []\n",
        "    \n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        steps = 0\n",
        "        while steps < 1000:\n",
        "            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            if not valid_actions:  # ê²Œì„ ì¢…ë£Œ (ì•ˆì „ì¥ì¹˜)\n",
        "                break\n",
        "                \n",
        "            action = agent.select_action(state, training=False, valid_actions=valid_actions)\n",
        "            next_state, _, done, info = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                total_scores.append(info['score'])\n",
        "                total_steps.append(steps)\n",
        "                highest_tiles.append(info['highest'])\n",
        "                break\n",
        "    \n",
        "    return {\n",
        "        'avg_score': np.mean(total_scores) if total_scores else 0,\n",
        "        'avg_steps': np.mean(total_steps) if total_steps else 0,\n",
        "        'avg_highest': np.mean(highest_tiles) if highest_tiles else 0,\n",
        "        'max_score': max(total_scores) if total_scores else 0,\n",
        "        'max_highest': max(highest_tiles) if highest_tiles else 0\n",
        "    }\n",
        "\n",
        "def train_agent(agent, env, monitor, episodes):\n",
        "    \"\"\"ì•¡ì…˜ ë§ˆìŠ¤í‚¹ì´ ì ìš©ëœ ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „)\"\"\"\n",
        "    print(f\"ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš© CNN ëª¨ë¸ í•™ìŠµ ì‹œì‘! (ëª©í‘œ: {episodes} ì—í”¼ì†Œë“œ)\")\n",
        "    start_time = time.time()\n",
        "    best_score = 0\n",
        "    best_highest = 0\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        episode_losses = []\n",
        "        final_info = {'score': 0, 'highest': 0, 'valid_actions': []}  # ê¸°ë³¸ê°’ ì„¤ì •\n",
        "\n",
        "        while steps < TRAINING_CONFIG['max_steps_per_episode']:\n",
        "            # ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš©\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            \n",
        "            # ìœ íš¨í•œ ì•¡ì…˜ì´ ì—†ìœ¼ë©´ ê²Œì„ ì¢…ë£Œ\n",
        "            if not valid_actions:\n",
        "                print(f\"Episode {episode+1}: No valid actions available at step {steps}\")\n",
        "                break\n",
        "            \n",
        "            # ì•¡ì…˜ ì„ íƒ (ìœ íš¨í•œ ì•¡ì…˜ë§Œ ê³ ë ¤)\n",
        "            action = agent.select_action(state, training=True, valid_actions=valid_actions)\n",
        "            \n",
        "            # í™˜ê²½ì—ì„œ ì•¡ì…˜ ì‹¤í–‰\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            final_info = info  # ë§ˆì§€ë§‰ info ì €ì¥\n",
        "            \n",
        "            # ê²½í—˜ ì €ì¥\n",
        "            if hasattr(agent, 'prioritized_replay') and agent.prioritized_replay and agent.memory.is_ready(agent.batch_size):\n",
        "                # TD error ë¯¸ë¦¬ ê³„ì‚°\n",
        "                with torch.no_grad():\n",
        "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
        "                    next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(agent.device)\n",
        "                    current_q = agent.q_network(state_tensor)[0][action]\n",
        "                    next_q = agent.target_network(next_state_tensor).max(1)[0]\n",
        "                    target_q = reward + (agent.gamma * next_q * (not done))\n",
        "                    td_error = abs((target_q - current_q).item())\n",
        "                    agent.store_experience(state, action, reward, next_state, done, td_error)\n",
        "            else:\n",
        "                agent.store_experience(state, action, reward, next_state, done)\n",
        "\n",
        "            # í•™ìŠµ ìˆ˜í–‰\n",
        "            if agent.memory.is_ready(agent.batch_size):\n",
        "                loss = agent.train_step()\n",
        "                if loss is not None:\n",
        "                    episode_losses.append(loss)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # ì—í”¼ì†Œë“œ í†µê³„ ì—…ë°ì´íŠ¸\n",
        "        avg_loss = np.mean(episode_losses) if episode_losses else None\n",
        "        monitor.add_episode(total_reward, final_info['score'], steps, avg_loss, final_info['highest'])\n",
        "        agent.episode_rewards.append(total_reward)\n",
        "\n",
        "        # ìµœê³  ê¸°ë¡ ê°±ì‹ \n",
        "        if final_info['score'] > best_score:\n",
        "            best_score = final_info['score']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_score.pth')\n",
        "            \n",
        "        if final_info['highest'] > best_highest:\n",
        "            best_highest = final_info['highest']\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_model_best_tile.pth')\n",
        "\n",
        "        # ì£¼ê¸°ì  ë¡œê¹…\n",
        "        if (episode + 1) % TRAINING_CONFIG['log_interval'] == 0:\n",
        "            stats = agent.get_stats()\n",
        "            elapsed = time.time() - start_time\n",
        "            loss_str = f\"{avg_loss:.4f}\" if avg_loss is not None else \"N/A\"\n",
        "            \n",
        "            print(f\"Episode {episode+1:4d} | Score: {final_info['score']:5.0f} | Highest: {final_info['highest']:4.0f} | \"\n",
        "                  f\"Reward: {total_reward:7.2f} | Steps: {steps:3d} | Valid: {len(final_info['valid_actions'])} | \"\n",
        "                  f\"Îµ: {stats['epsilon']:.3f} | Loss: {loss_str} | Time: {elapsed/60:.1f}min\")\n",
        "            \n",
        "        if (episode + 1) % TRAINING_CONFIG['eval_interval'] == 0:\n",
        "            eval_results = evaluate_agent(agent, env, TRAINING_CONFIG['eval_episodes'])\n",
        "            monitor.add_eval(episode + 1, eval_results['avg_score'])\n",
        "            \n",
        "            print(f\"ğŸ¯ í‰ê°€ ê²°ê³¼ (Episode {episode+1}):\")\n",
        "            print(f\"   í‰ê·  ì ìˆ˜: {eval_results['avg_score']:.1f}\")\n",
        "            print(f\"   ìµœê³  ì ìˆ˜: {eval_results['max_score']:.0f}\")\n",
        "            print(f\"   í‰ê·  ìµœê³ íƒ€ì¼: {eval_results['avg_highest']:.0f}\")\n",
        "            print(f\"   ìµœê³  íƒ€ì¼: {eval_results['max_highest']:.0f}\")\n",
        "\n",
        "        # ì£¼ê¸°ì  ì‹œê°í™”\n",
        "        if (episode + 1) % TRAINING_CONFIG['plot_interval'] == 0:\n",
        "            clear_output(wait=True)\n",
        "            monitor.plot_progress(f\"CNN Training Progress - Episode {episode+1}\")\n",
        "            \n",
        "            stats = monitor.get_stats()\n",
        "            print(f\" í˜„ì¬ í†µê³„ (ìµœê·¼ 100 ì—í”¼ì†Œë“œ):\")\n",
        "            for key, value in stats.items():\n",
        "                if isinstance(value, float):\n",
        "                    print(f\"  {key}: {value:.2f}\")\n",
        "                else:\n",
        "                    print(f\"  {key}: {value}\")\n",
        "            \n",
        "            print(f\"  í˜„ì¬ epsilon: {agent.get_epsilon():.4f}\")\n",
        "            print(f\"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {len(agent.memory):,}/{agent.memory.capacity:,}\")\n",
        "\n",
        "        # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥\n",
        "        if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
        "            agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_checkpoint_{episode+1}.pth')\n",
        "\n",
        "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "    agent.save_model(f'/content/drive/MyDrive/2048_models/cnn_final_action_masked.pth')\n",
        "    \n",
        "    print(f\"\\n ì•¡ì…˜ ë§ˆìŠ¤í‚¹ ì ìš© í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(f\" ìµœê³  ì ìˆ˜: {best_score}\")\n",
        "    print(f\" ìµœê³  íƒ€ì¼: {best_highest}\")\n",
        "    \n",
        "    return monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35c846c",
      "metadata": {
        "id": "f35c846c"
      },
      "source": [
        "## CNN ëª¨ë¸ í•™ìŠµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73bf4881",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "73bf4881",
        "outputId": "015c3dff-d775-45d5-cd1a-7f201d50c56b"
      },
      "outputs": [],
      "source": [
        "monitor = train_agent(\n",
        "    agent=agent,\n",
        "    env=env,\n",
        "    monitor=monitor,\n",
        "    episodes=TRAINING_CONFIG['episodes']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0191a35b",
      "metadata": {
        "id": "0191a35b"
      },
      "source": [
        "## ìµœì¢… ì„±ëŠ¥ í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d47bca33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class ModelPerformanceEvaluator:\n",
        "    def __init__(self, agent, env):\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        \n",
        "    def evaluate_comprehensive(self, num_games=100):\n",
        "        \"\"\"í¬ê´„ì ì¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "        print(f\"Final Model Evaluation Started ({num_games} games)\")\n",
        "        \n",
        "        results = {\n",
        "            'scores': [],\n",
        "            'highest_tiles': [],\n",
        "            'steps': [],\n",
        "            'game_details': []  # ìµœê³ /ìµœì•… ê²Œì„ ë¶„ì„ìš©\n",
        "        }\n",
        "        \n",
        "        for game_idx in range(num_games):\n",
        "            game_result = self._play_single_game()\n",
        "            \n",
        "            results['scores'].append(game_result['final_score'])\n",
        "            results['highest_tiles'].append(game_result['highest_tile'])\n",
        "            results['steps'].append(game_result['steps'])\n",
        "            results['game_details'].append(game_result)\n",
        "            \n",
        "            if (game_idx + 1) % 25 == 0:\n",
        "                print(f\"  Progress: {game_idx + 1}/{num_games} games completed\")\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _play_single_game(self):\n",
        "        \"\"\"ë‹¨ì¼ ê²Œì„ í”Œë ˆì´\"\"\"\n",
        "        state = self.env.reset()\n",
        "        steps = 0\n",
        "        \n",
        "        while steps < 2000:\n",
        "            valid_actions = self.env.get_valid_actions()\n",
        "            if not valid_actions:\n",
        "                break\n",
        "                \n",
        "            action = self.agent.select_action(state, training=False, valid_actions=valid_actions)\n",
        "            next_state, reward, done, info = self.env.step(action)\n",
        "            \n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        return {\n",
        "            'final_score': info['score'],\n",
        "            'highest_tile': info['highest'],\n",
        "            'steps': steps,\n",
        "            'final_board': self.env.get_board().copy()\n",
        "        }\n",
        "    \n",
        "    def analyze_and_visualize(self, results):\n",
        "        \"\"\"í•µì‹¬ ë¶„ì„ ë° ì‹œê°í™”\"\"\"\n",
        "        scores = np.array(results['scores'])\n",
        "        highest_tiles = np.array(results['highest_tiles'])\n",
        "        \n",
        "        # 1. íƒ€ì¼ë³„ ë‹¬ì„±ë¥  ë¶„ì„\n",
        "        tile_achievements = Counter(highest_tiles)\n",
        "        print(\"\\n Tile Achievement Rates:\")\n",
        "        total_games = len(scores)\n",
        "        for tile in sorted(tile_achievements.keys(), reverse=True):\n",
        "            count = tile_achievements[tile]\n",
        "            percentage = (count / total_games) * 100\n",
        "            print(f\"  {tile:4d} tile: {count:3d} games ({percentage:5.1f}%)\")\n",
        "        \n",
        "        # 2. ìµœê³ /ìµœì•… ê²Œì„ ì°¾ê¸°\n",
        "        best_idx = np.argmax(scores)\n",
        "        worst_idx = np.argmin(scores)\n",
        "        \n",
        "        best_game = results['game_details'][best_idx]\n",
        "        worst_game = results['game_details'][worst_idx]\n",
        "        \n",
        "        # 3. ì‹œê°í™”\n",
        "        self._plot_analysis(tile_achievements, best_game, worst_game, scores)\n",
        "        \n",
        "        # 4. ìš”ì•½ í†µê³„\n",
        "        print(f\"\\n Performance Summary:\")\n",
        "        print(f\"  Average Score: {np.mean(scores):.1f} Â± {np.std(scores):.1f}\")\n",
        "        print(f\"  Best Score: {np.max(scores):.0f} (Highest Tile: {best_game['highest_tile']})\")\n",
        "        print(f\"  Worst Score: {np.min(scores):.0f} (Highest Tile: {worst_game['highest_tile']})\")\n",
        "        print(f\"  Average Highest Tile: {np.mean(highest_tiles):.1f}\")\n",
        "    \n",
        "    def _plot_analysis(self, tile_achievements, best_game, worst_game, scores):\n",
        "        \"\"\"í•µì‹¬ ì‹œê°í™”\"\"\"\n",
        "        # 3ê°œ ì„œë¸Œí”Œë¡¯ í¬ê¸° ë™ì¼í•˜ê²Œ ì„¤ì •\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        \n",
        "        # 1. íƒ€ì¼ ë‹¬ì„±ë¥ \n",
        "        plt.subplot(1, 3, 1)\n",
        "        tiles = sorted(tile_achievements.keys(), reverse=True)\n",
        "        counts = [tile_achievements[tile] for tile in tiles]\n",
        "        total_games = sum(counts)\n",
        "        percentages = [(count / total_games) * 100 for count in counts]\n",
        "        \n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(tiles)))\n",
        "        bars = plt.bar(range(len(tiles)), percentages, color=colors)\n",
        "        plt.title('Highest Tile Achievement Rate')\n",
        "        plt.xlabel('Highest Tile')\n",
        "        plt.ylabel('Achievement Rate (%)')\n",
        "        plt.xticks(range(len(tiles)), [str(tile) for tile in tiles], rotation=45)\n",
        "        \n",
        "        # ë¹„ìœ¨ í‘œì‹œ\n",
        "        for i, (bar, count, pct) in enumerate(zip(bars, counts, percentages)):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5, \n",
        "                    f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # 2. ìµœê³  ê²Œì„ ë³´ë“œ\n",
        "        plt.subplot(1, 3, 2)\n",
        "        self._plot_board(best_game['final_board'], \"Best Game\")\n",
        "        # ì ìˆ˜ì™€ ìµœê³  íƒ€ì¼ ì •ë³´ë¥¼ ì‚´ì§ ì˜¬ë ¤ì„œ í‘œì‹œ\n",
        "        plt.text(0.5, -0.10, f\"Score: {best_game['final_score']}\\nHighest: {best_game['highest_tile']}\", \n",
        "                ha='center', va='top', transform=plt.gca().transAxes, fontsize=10, color='black')\n",
        "        \n",
        "        # 3. ìµœì•… ê²Œì„ ë³´ë“œ\n",
        "        plt.subplot(1, 3, 3)\n",
        "        self._plot_board(worst_game['final_board'], \"Worst Game\")\n",
        "        # ì ìˆ˜ì™€ ìµœê³  íƒ€ì¼ ì •ë³´ë¥¼ ì‚´ì§ ì˜¬ë ¤ì„œ í‘œì‹œ\n",
        "        plt.text(0.5, -0.10, f\"Score: {worst_game['final_score']}\\nHighest: {worst_game['highest_tile']}\", \n",
        "                ha='center', va='top', transform=plt.gca().transAxes, fontsize=10, color='black')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def _get_tile_color(self, tile_value):\n",
        "        \"\"\"2048 ê²Œì„ ì „í†µ ìƒ‰ìƒ ë§¤í•‘\"\"\"\n",
        "        color_map = {\n",
        "            0: '#CDC1B4',    # ë¹ˆ ê³µê°„ (íšŒìƒ‰)\n",
        "            2: '#EEE4DA',    # Light yellow/beige\n",
        "            4: '#EDE0C8',    # Light orange/yellow\n",
        "            8: '#F2B179',    # Darker orange\n",
        "            16: '#F59563',   # Orange\n",
        "            32: '#F67C5F',   # Red-orange\n",
        "            64: '#F65E3B',   # Red\n",
        "            128: '#EDCF72',  # Orange/red shade (237, 207, 114)\n",
        "            256: '#EDCC61',  # Orange/red shade (237, 204, 97)\n",
        "            512: '#EDC850',  # Orange/red shade (237, 200, 80)\n",
        "            1024: '#EDC53F', # Orange/red shade (237, 197, 63)\n",
        "            2048: '#EDC22E', # Orange/red shade (237, 194, 46)\n",
        "            4096: '#000000', # Black\n",
        "        }\n",
        "        \n",
        "        # ì •ì˜ëœ ìƒ‰ìƒì´ ì—†ìœ¼ë©´ ê°€ì¥ ë†’ì€ ìƒ‰ìƒ ì‚¬ìš©\n",
        "        if tile_value in color_map:\n",
        "            return color_map[tile_value]\n",
        "        elif tile_value > 2048:\n",
        "            return color_map[4096]\n",
        "        else:\n",
        "            return color_map[0]\n",
        "    \n",
        "    def _plot_board(self, board, title):\n",
        "        \"\"\"ë³´ë“œ ì‹œê°í™” (íƒ€ì¼ë³„ ê³ ì • ìƒ‰ìƒ)\"\"\"\n",
        "        # 4x4 ìƒ‰ìƒ ë°°ì—´ ìƒì„±\n",
        "        color_board = np.zeros((4, 4, 3))\n",
        "        \n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                tile_value = int(board[i, j])\n",
        "                hex_color = self._get_tile_color(tile_value)\n",
        "                # hexë¥¼ RGBë¡œ ë³€í™˜\n",
        "                rgb = tuple(int(hex_color[k:k+2], 16)/255.0 for k in (1, 3, 5))\n",
        "                color_board[i, j] = rgb\n",
        "        \n",
        "        plt.imshow(color_board, aspect='equal')\n",
        "        plt.title(title, fontsize=12, pad=15)  # íŒ¨ë”© ì¶”ê°€\n",
        "        \n",
        "        # ê° ì…€ì— ì‹¤ì œ ê°’ í‘œì‹œ (í°íŠ¸ í¬ê¸° ì¦ê°€)\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                tile_value = int(board[i, j])\n",
        "                if tile_value > 0:\n",
        "                    # í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê²°ì • (íƒ€ì¼ ìƒ‰ìƒì— ë”°ë¼)\n",
        "                    text_color = 'white' if tile_value >= 8 else 'black'\n",
        "                    # í°íŠ¸ í¬ê¸° ì¡°ì • (ê¸°ì¡´ë³´ë‹¤ 2pt ì¦ê°€)\n",
        "                    font_size = 14 if tile_value < 1000 else 12\n",
        "                    \n",
        "                    plt.text(j, i, str(tile_value), ha='center', va='center', \n",
        "                           fontsize=font_size, fontweight='bold', color=text_color)\n",
        "        \n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "# ======================== í‰ê°€ ì‹¤í–‰ ========================\n",
        "\n",
        "print(\" ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘...\")\n",
        "evaluator = ModelPerformanceEvaluator(agent, env)\n",
        "\n",
        "# 100ê²Œì„ í‰ê°€\n",
        "results = evaluator.evaluate_comprehensive(num_games=100)\n",
        "\n",
        "# ë¶„ì„ ë° ì‹œê°í™”\n",
        "evaluator.analyze_and_visualize(results)\n",
        "\n",
        "print(\"\\n ëª¨ë¸ í‰ê°€ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fff8962",
      "metadata": {
        "id": "0fff8962"
      },
      "source": [
        "## ONNX ë³€í™˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0b97a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0b97a7",
        "outputId": "12ccaca5-9101-418c-d0d5-4675fa248d0d"
      },
      "outputs": [],
      "source": [
        "print(\" ONNX ë³€í™˜ ì‹œì‘...\")\n",
        "agent.export_to_onnx(\n",
        "    filepath='/content/drive/MyDrive/2048_models/cnn_model.onnx',\n",
        "    input_shape=(4, 4, 16)\n",
        ")\n",
        "print(\" ONNX ë³€í™˜ ì™„ë£Œ!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
