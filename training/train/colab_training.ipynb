{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70268a27",
   "metadata": {},
   "source": [
    "# 2048 DQN 모델 학습\n",
    "\n",
    "## 학습 계획\n",
    "- **목적**: 2048 게임에서 고득점을 달성하는 DQN 에이전트 학습\n",
    "- **아키텍처**: CNN (Layered) vs DNN (Flat) 비교\n",
    "- **환경**: Google Colab GPU 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f131c",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "!pip install torch torchvision gym matplotlib seaborn tensorboard\n",
    "!pip install onnx onnxruntime\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa566b",
   "metadata": {},
   "source": [
    "## 코드 업로드 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e40528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 프로젝트 경로 설정\n",
    "PROJECT_ROOT = '/content/drive/MyDrive/2048-rl-project'\n",
    "TRAINING_PATH = os.path.join(PROJECT_ROOT, 'training')\n",
    "\n",
    "# Python 경로에 추가\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "if TRAINING_PATH not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_PATH)\n",
    "\n",
    "# 작업 디렉토리 변경\n",
    "os.chdir(TRAINING_PATH)\n",
    "\n",
    "# __init__.py 파일들이 존재하는지 확인\n",
    "init_files = [\n",
    "    os.path.join(PROJECT_ROOT, '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'models', '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'environment', '__init__.py'),\n",
    "    os.path.join(TRAINING_PATH, 'train', '__init__.py')\n",
    "]\n",
    "\n",
    "print(\"📁 __init__.py 파일 확인:\")\n",
    "for init_file in init_files:\n",
    "    exists = \"✅\" if os.path.exists(init_file) else \"❌\"\n",
    "    print(f\"  {exists} {init_file}\")\n",
    "\n",
    "print(f\"\\n✅ 경로 설정 완료!\")\n",
    "print(f\"  프로젝트 루트: {PROJECT_ROOT}\")\n",
    "print(f\"  학습 경로: {TRAINING_PATH}\")\n",
    "print(f\"  현재 작업 디렉토리: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d7c58",
   "metadata": {},
   "source": [
    "## 모델 및 환경 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import 및 테스트\n",
    "print(\"🔄 모듈 로딩 시작...\")\n",
    "\n",
    "try:\n",
    "    from environment.game_2048 import Game2048Env\n",
    "    print(\"✅ Game2048Env 로드 성공\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Game2048Env 로드 실패: {e}\")\n",
    "\n",
    "try:\n",
    "    from models.dqn_agent import DQNAgent\n",
    "    print(\"✅ DQNAgent 로드 성공\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ DQNAgent 로드 실패: {e}\")\n",
    "\n",
    "try:\n",
    "    from models.networks import count_parameters\n",
    "    print(\"✅ Networks 로드 성공\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Networks 로드 실패: {e}\")\n",
    "\n",
    "# 기타 필요한 라이브러리\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "import pandas as pd\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 간단한 동작 테스트\n",
    "print(\"\\n🧪 동작 테스트:\")\n",
    "try:\n",
    "    # 환경 생성 테스트\n",
    "    test_env = Game2048Env(observation_type='flat')\n",
    "    print(f\"✅ 환경 생성 테스트 성공: {test_env.observation_space}\")\n",
    "    \n",
    "    # 에이전트 생성 테스트\n",
    "    test_agent = DQNAgent(observation_type='flat', buffer_size=1000, batch_size=32)\n",
    "    print(f\"✅ 에이전트 생성 테스트 성공: {count_parameters(test_agent.q_network):,} 파라미터\")\n",
    "    \n",
    "    print(\"\\n🎉 모든 모듈 로드 및 테스트 완료!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 테스트 실패: {e}\")\n",
    "    print(\"📋 문제 해결을 위해 다음을 확인하세요:\")\n",
    "    print(\"  1. 모든 파일이 올바른 경로에 있는지\")\n",
    "    print(\"  2. __init__.py 파일들이 모두 생성되었는지\")\n",
    "    print(\"  3. import 경로가 올바른지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de60ca",
   "metadata": {},
   "source": [
    "## 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb624383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 하이퍼파라미터\n",
    "TRAINING_CONFIG = {\n",
    "    # 기본 설정\n",
    "    'episodes': 2000,\n",
    "    'max_steps_per_episode': 1000,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # DQN 설정\n",
    "    'buffer_size': 100000,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-4,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_end': 0.01,\n",
    "    'epsilon_decay': 50000,\n",
    "    'target_update': 1000,\n",
    "    \n",
    "    # 평가 설정\n",
    "    'eval_interval': 100,\n",
    "    'eval_episodes': 10,\n",
    "    'save_interval': 500,\n",
    "    \n",
    "    # 시각화 설정\n",
    "    'plot_interval': 50,\n",
    "    'log_interval': 10\n",
    "}\n",
    "\n",
    "print(\"⚙️ 학습 설정 완료:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94975f",
   "metadata": {},
   "source": [
    "## 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_and_env(observation_type='layered'):\n",
    "    \"\"\"에이전트와 환경 생성\"\"\"\n",
    "    env = Game2048Env(observation_type=observation_type)\n",
    "    \n",
    "    agent = DQNAgent(\n",
    "        observation_type=observation_type,\n",
    "        lr=TRAINING_CONFIG['lr'],\n",
    "        gamma=TRAINING_CONFIG['gamma'],\n",
    "        epsilon_start=TRAINING_CONFIG['epsilon_start'],\n",
    "        epsilon_end=TRAINING_CONFIG['epsilon_end'],\n",
    "        epsilon_decay=TRAINING_CONFIG['epsilon_decay'],\n",
    "        buffer_size=TRAINING_CONFIG['buffer_size'],\n",
    "        batch_size=TRAINING_CONFIG['batch_size'],\n",
    "        target_update=TRAINING_CONFIG['target_update'],\n",
    "        device=TRAINING_CONFIG['device']\n",
    "    )\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# CNN과 DNN 에이전트 생성\n",
    "print(\"🧠 CNN 에이전트 (Layered) 생성...\")\n",
    "cnn_agent, cnn_env = create_agent_and_env('layered')\n",
    "\n",
    "print(\"\\n🧠 DNN 에이전트 (Flat) 생성...\")\n",
    "dnn_agent, dnn_env = create_agent_and_env('flat')\n",
    "\n",
    "print(f\"\\n📊 모델 비교:\")\n",
    "print(f\"  CNN 파라미터: {count_parameters(cnn_agent.q_network):,}\")\n",
    "print(f\"  DNN 파라미터: {count_parameters(dnn_agent.q_network):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e08891f",
   "metadata": {},
   "source": [
    "## 학습 모니터링 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faa7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_rewards = []\n",
    "        self.episode_scores = []\n",
    "        self.episode_steps = []\n",
    "        self.episode_losses = []\n",
    "        self.highest_tiles = []\n",
    "        self.eval_scores = []\n",
    "        self.eval_episodes = []\n",
    "    \n",
    "    def add_episode(self, reward, score, steps, loss, highest_tile):\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_scores.append(score)\n",
    "        self.episode_steps.append(steps)\n",
    "        self.episode_losses.append(loss)\n",
    "        self.highest_tiles.append(highest_tile)\n",
    "    \n",
    "    def add_eval(self, episode, avg_score):\n",
    "        self.eval_episodes.append(episode)\n",
    "        self.eval_scores.append(avg_score)\n",
    "    \n",
    "    def plot_progress(self, title=\"Training Progress\"):\n",
    "        if len(self.episode_rewards) < 10:\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        \n",
    "        # 보상\n",
    "        axes[0,0].plot(self.episode_rewards, alpha=0.3, color='blue')\n",
    "        axes[0,0].plot(pd.Series(self.episode_rewards).rolling(50).mean(), color='red')\n",
    "        axes[0,0].set_title('Episode Rewards')\n",
    "        axes[0,0].set_ylabel('Reward')\n",
    "        \n",
    "        # 점수\n",
    "        axes[0,1].plot(self.episode_scores, alpha=0.3, color='green')\n",
    "        axes[0,1].plot(pd.Series(self.episode_scores).rolling(50).mean(), color='red')\n",
    "        axes[0,1].set_title('Episode Scores')\n",
    "        axes[0,1].set_ylabel('Score')\n",
    "        \n",
    "        # 최고 타일\n",
    "        axes[0,2].plot(self.highest_tiles, alpha=0.3, color='purple')\n",
    "        axes[0,2].plot(pd.Series(self.highest_tiles).rolling(50).mean(), color='red')\n",
    "        axes[0,2].set_title('Highest Tiles')\n",
    "        axes[0,2].set_ylabel('Tile Value')\n",
    "        \n",
    "        # 스텝 수\n",
    "        axes[1,0].plot(self.episode_steps, alpha=0.3, color='orange')\n",
    "        axes[1,0].plot(pd.Series(self.episode_steps).rolling(50).mean(), color='red')\n",
    "        axes[1,0].set_title('Episode Steps')\n",
    "        axes[1,0].set_ylabel('Steps')\n",
    "        axes[1,0].set_xlabel('Episode')\n",
    "        \n",
    "        # 손실\n",
    "        if self.episode_losses and any(loss is not None for loss in self.episode_losses):\n",
    "            valid_losses = [l for l in self.episode_losses if l is not None]\n",
    "            if valid_losses:\n",
    "                axes[1,1].plot(valid_losses, alpha=0.3, color='red')\n",
    "                axes[1,1].plot(pd.Series(valid_losses).rolling(20).mean(), color='darkred')\n",
    "        axes[1,1].set_title('Training Loss')\n",
    "        axes[1,1].set_ylabel('Loss')\n",
    "        axes[1,1].set_xlabel('Episode')\n",
    "        \n",
    "        # 평가 점수\n",
    "        if self.eval_scores:\n",
    "            axes[1,2].plot(self.eval_episodes, self.eval_scores, 'o-', color='darkgreen')\n",
    "        axes[1,2].set_title('Evaluation Scores')\n",
    "        axes[1,2].set_ylabel('Avg Score')\n",
    "        axes[1,2].set_xlabel('Episode')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        if not self.episode_scores:\n",
    "            return {}\n",
    "        \n",
    "        recent_scores = self.episode_scores[-100:] if len(self.episode_scores) >= 100 else self.episode_scores\n",
    "        recent_tiles = self.highest_tiles[-100:] if len(self.highest_tiles) >= 100 else self.highest_tiles\n",
    "        \n",
    "        return {\n",
    "            'episodes': len(self.episode_scores),\n",
    "            'avg_score': np.mean(recent_scores),\n",
    "            'max_score': max(self.episode_scores),\n",
    "            'avg_highest_tile': np.mean(recent_tiles),\n",
    "            'max_highest_tile': max(self.highest_tiles),\n",
    "            'avg_steps': np.mean(self.episode_steps[-100:]) if len(self.episode_steps) >= 100 else np.mean(self.episode_steps)\n",
    "        }\n",
    "\n",
    "# 모니터 초기화\n",
    "cnn_monitor = TrainingMonitor()\n",
    "dnn_monitor = TrainingMonitor()\n",
    "\n",
    "print(\"📊 학습 모니터링 시스템 준비 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8e403",
   "metadata": {},
   "source": [
    "## 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, monitor, name, episodes):\n",
    "    \"\"\"에이전트 학습 함수\"\"\"\n",
    "    print(f\"🚀 {name} 학습 시작! (목표: {episodes} 에피소드)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_score = 0\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        while steps < TRAINING_CONFIG['max_steps_per_episode']:\n",
    "            action = agent.select_action(state, training=True)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                loss = agent.train_step()\n",
    "                if loss is not None:\n",
    "                    episode_losses.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 에피소드 통계 기록\n",
    "        avg_loss = np.mean(episode_losses) if episode_losses else None\n",
    "        monitor.add_episode(total_reward, info['score'], steps, avg_loss, info['highest'])\n",
    "        agent.episode_rewards.append(total_reward)\n",
    "        \n",
    "        # 베스트 모델 저장\n",
    "        if info['score'] > best_score:\n",
    "            best_score = info['score']\n",
    "            try:\n",
    "                agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_best.pth')\n",
    "            except Exception as save_error:\n",
    "                print(f\"⚠️ 모델 저장 실패: {save_error}\")\n",
    "        \n",
    "        # 로그 출력 - 수정된 부분\n",
    "        if (episode + 1) % TRAINING_CONFIG['log_interval'] == 0:\n",
    "            try:\n",
    "                stats = agent.get_stats()\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # 손실 값을 미리 포맷팅 (안전한 방법)\n",
    "                if avg_loss is not None:\n",
    "                    loss_str = f\"{avg_loss:.3f}\"\n",
    "                else:\n",
    "                    loss_str = \"N/A\"\n",
    "                \n",
    "                print(f\"{name} Episode {episode+1:4d} | \"\n",
    "                      f\"Score: {info['score']:4.0f} | \"\n",
    "                      f\"Reward: {total_reward:6.1f} | \"\n",
    "                      f\"Steps: {steps:3d} | \"\n",
    "                      f\"Epsilon: {stats['epsilon']:.3f} | \"\n",
    "                      f\"Loss: {loss_str} | \"\n",
    "                      f\"Time: {elapsed/60:.1f}min\")\n",
    "                      \n",
    "            except Exception as log_error:\n",
    "                print(f\"⚠️ 로그 출력 에러: {log_error}\")\n",
    "        \n",
    "        # 평가 및 시각화\n",
    "        if (episode + 1) % TRAINING_CONFIG['eval_interval'] == 0:\n",
    "            try:\n",
    "                eval_score = evaluate_agent(agent, env, TRAINING_CONFIG['eval_episodes'])\n",
    "                monitor.add_eval(episode + 1, eval_score)\n",
    "                print(f\"🎯 {name} Evaluation (Episode {episode+1}): {eval_score:.1f}\")\n",
    "            except Exception as eval_error:\n",
    "                print(f\"⚠️ 평가 에러: {eval_error}\")\n",
    "        \n",
    "        if (episode + 1) % TRAINING_CONFIG['plot_interval'] == 0:\n",
    "            try:\n",
    "                clear_output(wait=True)\n",
    "                monitor.plot_progress(f\"{name} Training Progress\")\n",
    "                \n",
    "                stats = monitor.get_stats()\n",
    "                print(f\"\\n📊 {name} 현재 통계:\")\n",
    "                \n",
    "                # 안전한 통계 출력 - 수정된 부분\n",
    "                for key, value in stats.items():\n",
    "                    if isinstance(value, float):\n",
    "                        print(f\"  {key}: {value:.2f}\")\n",
    "                    elif isinstance(value, int):\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                        \n",
    "            except Exception as plot_error:\n",
    "                print(f\"⚠️ 시각화 에러: {plot_error}\")\n",
    "        \n",
    "        # 정기 저장\n",
    "        if (episode + 1) % TRAINING_CONFIG['save_interval'] == 0:\n",
    "            try:\n",
    "                agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_checkpoint_{episode+1}.pth')\n",
    "            except Exception as checkpoint_error:\n",
    "                print(f\"⚠️ 체크포인트 저장 실패: {checkpoint_error}\")\n",
    "    \n",
    "    # 최종 저장\n",
    "    try:\n",
    "        agent.save_model(f'/content/drive/MyDrive/2048_models/{name}_final.pth')\n",
    "        print(f\"✅ {name} 학습 완료!\")\n",
    "    except Exception as final_save_error:\n",
    "        print(f\"⚠️ 최종 모델 저장 실패: {final_save_error}\")\n",
    "        print(f\"✅ {name} 학습 완료! (저장 실패)\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "print(\"🔧 학습 함수 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c846c",
   "metadata": {},
   "source": [
    "## CNN 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 학습 실행\n",
    "cnn_monitor = train_agent(\n",
    "    agent=cnn_agent,\n",
    "    env=cnn_env,\n",
    "    monitor=cnn_monitor,\n",
    "    name=\"CNN\",\n",
    "    episodes=TRAINING_CONFIG['episodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef46be8",
   "metadata": {},
   "source": [
    "## DNN 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26506513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 학습 실행\n",
    "dnn_monitor = train_agent(\n",
    "    agent=dnn_agent,\n",
    "    env=dnn_env,\n",
    "    monitor=dnn_monitor,\n",
    "    name=\"DNN\",\n",
    "    episodes=TRAINING_CONFIG['episodes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191a35b",
   "metadata": {},
   "source": [
    "## 🏆 최종 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff302daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 평가\n",
    "print(\"🏆 최종 성능 평가 (각 50 게임)\")\n",
    "\n",
    "cnn_final_score = evaluate_agent(cnn_agent, cnn_env, 50)\n",
    "dnn_final_score = evaluate_agent(dnn_agent, dnn_env, 50)\n",
    "\n",
    "print(f\"\\n📊 최종 결과:\")\n",
    "print(f\"  CNN 평균 점수: {cnn_final_score:.1f}\")\n",
    "print(f\"  DNN 평균 점수: {dnn_final_score:.1f}\")\n",
    "\n",
    "winner = \"CNN\" if cnn_final_score > dnn_final_score else \"DNN\"\n",
    "diff = abs(cnn_final_score - dnn_final_score)\n",
    "print(f\"\\n🏆 승자: {winner} (평균 {diff:.1f}점 차이)\")\n",
    "\n",
    "# 통계 비교\n",
    "cnn_stats = cnn_monitor.get_stats()\n",
    "dnn_stats = dnn_monitor.get_stats()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'CNN': [cnn_stats.get(k, 0) for k in ['avg_score', 'max_score', 'max_highest_tile', 'avg_steps']],\n",
    "    'DNN': [dnn_stats.get(k, 0) for k in ['avg_score', 'max_score', 'max_highest_tile', 'avg_steps']]\n",
    "}, index=['평균 점수', '최고 점수', '최고 타일', '평균 스텝'])\n",
    "\n",
    "print(\"\\n📈 상세 비교:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# 최종 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# CNN 결과\n",
    "axes[0].plot(cnn_monitor.episode_scores, alpha=0.3, label='Episode Scores')\n",
    "axes[0].plot(pd.Series(cnn_monitor.episode_scores).rolling(100).mean(), label='Moving Average (100)')\n",
    "axes[0].set_title('CNN Training Progress')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "\n",
    "# DNN 결과\n",
    "axes[1].plot(dnn_monitor.episode_scores, alpha=0.3, label='Episode Scores')\n",
    "axes[1].plot(pd.Series(dnn_monitor.episode_scores).rolling(100).mean(), label='Moving Average (100)')\n",
    "axes[1].set_title('DNN Training Progress')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff8962",
   "metadata": {},
   "source": [
    "## ONNX 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 베스트 모델 ONNX 변환\n",
    "print(\"🔄 ONNX 변환 시작...\")\n",
    "\n",
    "# CNN 모델 변환\n",
    "cnn_agent.export_to_onnx(\n",
    "    filepath='/content/drive/MyDrive/2048_models/cnn_model.onnx',\n",
    "    input_shape=(4, 4, 16)\n",
    ")\n",
    "\n",
    "# DNN 모델 변환\n",
    "dnn_agent.export_to_onnx(\n",
    "    filepath='/content/drive/MyDrive/2048_models/dnn_model.onnx',\n",
    "    input_shape=(16,)\n",
    ")\n",
    "\n",
    "print(\"✅ ONNX 변환 완료!\")\n",
    "print(\"  - CNN: /content/drive/MyDrive/2048_models/cnn_model.onnx\")\n",
    "print(\"  - DNN: /content/drive/MyDrive/2048_models/dnn_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf49f2",
   "metadata": {},
   "source": [
    "## 데모 플레이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델로 게임 플레이 데모\n",
    "def demo_play(agent, env, name, render_interval=10):\n",
    "    print(f\"🎮 {name} 데모 플레이 시작!\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 1000:\n",
    "        if steps % render_interval == 0:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            print(f\"Step: {steps}\")\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        action = agent.select_action(state, training=False)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        steps += 1\n",
    "    \n",
    "    env.render()\n",
    "    print(f\"\\n🏁 {name} 게임 종료!\")\n",
    "    print(f\"  최종 점수: {info['score']}\")\n",
    "    print(f\"  최고 타일: {info['highest']}\")\n",
    "    print(f\"  총 스텝: {steps}\")\n",
    "\n",
    "# 베스트 모델로 데모\n",
    "best_agent = cnn_agent if cnn_final_score > dnn_final_score else dnn_agent\n",
    "best_env = cnn_env if cnn_final_score > dnn_final_score else dnn_env\n",
    "best_name = \"CNN\" if cnn_final_score > dnn_final_score else \"DNN\"\n",
    "\n",
    "demo_play(best_agent, best_env, f\"Best {best_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
